,Body,Tag
0,"sara ford has this market cornered.  http://blogs.msdn.com/saraford/default.aspx  more visual studio tips and tricks than you can shake a stick at.  some others:   the visual studio 2005 and 2008 3-month trial editions are fully-functional, and can be used indefinitely (forever) by setting the system clock back prior to opening vs. then, when vs is opened, set the system clock forward again so your datetimes aren't screwed up. but that's really piracy and i can't recommend it, especially when anybody with a .edu address can get a fully-functional pro version of vs2008 through microsoft dreamspark. you can use visual studio to open 3rd-party executables, and browse embedded resources (dialogs, string tables, images, etc) stored within. debugging visualizers are not exactly a ""hidden"" feature but they are somewhat neglected, and super-useful, since in addition to using the provided visualizers you can roll your own for specific data sets. debugger's ""set instruction pointer"" or ""set next statement"" command. conditional breakpoints (as kiwibastard noted). you can use quickwatch etc. to evaluate not only the value of a variable, but runtime expressions around that variable.  ",visual-studio text-editor hidden-features
1,the irc specification is laid out in rfc 1459  http://www.irchelp.org/irchelp/rfc/rfc.html ,ruby shoes irc
2,"   the real world isn't ""oo"", and the idea implicit in oo--that we can model things with some class taxonomy--seems to me very fundamentally flawed   while this is true and has been observed by other people (take stepanov, inventor of the stl), the rest is nonsense. oop may be flawed and it certainly is no silver bullet but it makes large-scale applications much simpler because it's a great way to reduce dependencies. of course, this is only true for âgoodâ oop design. sloppy design won't give any advantage. but good, decoupled design can be modelled very well using oop and not well using other techniques.  there are much better, more universal models (haskell's type model comes to mind) but these are also often more complicated and/or difficult to implement efficiently. oop is a good trade-off between extremes. ",language-agnostic oop
3,"you're already using sqlbulkcopy, which is a good start.  however, just using the sqlbulkcopy class does not necessarily mean that sql will perform a bulk copy. in particular, there are a few requirements that must be met for sql server to perform an efficient bulk insert.  further reading:   prerequisites for minimal logging in bulk import optimizing bulk import performance   out of curiosity, why is your index set up like that  it seems like containerid/binid/sequence is much better suited to be a nonclustered index. is there a particular reason you wanted this index to be clustered  ",c# sql sql-server sql-server-2005
4,"the ""class"" that you've constructed above is what most people would use a struct for in other languages. i'm not sure what the performance implications are in php, though i suspect instantiating the objects is probably more costly here, if only by a little bit.  that being said, if the cost is relatively low, it is a bit easier to manage the objects, in my opinion.  i'm only saying the following based on the title and your question, but: bear in mind that classes provide the advantage of methods and access control, as well. so if you wanted to ensure that people weren't changing weights to negative numbers, you could make the weight field private and provide some accessor methods, like getweight() and setweight(). inside setweight(), you could do some value checking, like so:  public function setweight($weight) {     if($weight &gt;= 0)     {         $this-&gt;weight = $weight;     }     else     {         // handle this scenario however you like     } }  ",php arrays class
5,"so far, from what i have seen, rmc seems to be one-of-a-kind, which is both cool, but sad :(  it seems pretty inactive, but i have posted a couple of questions on there, and amazed at the response (a lot of people just have the rss feed and wait for something to happen). so i would definately recommend just getting active on there..  however, i have read some great books on safari including ms press' ""test driven development"" which i found really helped . however i have to admit, i still need a lot of work in adjusting my brain for tdd etc.  following this question though  (+1) look forward to seeing if there is more out there  :)  edit:  just found this blog post that outlines a number of good bits of software, websites and books :) i knew i had one somewhere  ",unit-testing testing
6,try ruby2ruby ,ruby serialization lambda proc-object
7,"class producer implements runnable {    private final blockingqueue queue;    producer(blockingqueue q) { queue = q; }    public void run() {      try {        while (true) { queue.put(produce()); }      } catch (interruptedexception ex) { ... handle ...}    }    object produce() { ... }  }   class consumer implements runnable {    private final blockingqueue queue;    consumer(blockingqueue q) { queue = q; }    public void run() {      try {        while (true) { consume(queue.take()); }      } catch (interruptedexception ex) { ... handle ...}    }    void consume(object x) { ... }  }   class setup {    void main() {      blockingqueue q = new somequeueimplementation();      producer p = new producer(q);      consumer c1 = new consumer(q);      consumer c2 = new consumer(q);      new thread(p).start();      new thread(c1).start();      new thread(c2).start();    }  }   this example was taken from the jdk 1.6 docs of blockingqueue. so you can see that you are doing it the right way. here's the quote which tells you that it have to work:     memory consistency effects: as with   other concurrent collections, actions   in a thread prior to placing an object   into a blockingqueue happen-before   actions subsequent to the access or   removal of that element from the   blockingqueue in another thread.  ",java concurrency
8,lolcode:  sorry i couldn't resist xd  hai can has stdio  i has a var i has a int i has a cheezburger i has a factorialnum im in yr loop     up var  1     tiemzd int  [cheezburger]     up factorialnum  1     iz var bigger than factorialnum  gtfo im outta yr loop u seez int kthxbye  ,algorithm language-agnostic
9,"one strategy you could use is mvcc, multi-value concurrency control.  in this scheme, you never do updates to any of your tables, you just do inserts, maintaining version numbers for each record.  this has the advantage of providing an exact snapshot from any point in time, and it also completely sidesteps the update lock problems that plague many databases.  but it makes for a huge database, and selects all require an extra clause to select the current version of a record. ",database database-design postgresql
10,"i know a few of the developers on the carleton university developed blindside project.  they are actively developing an open-source web conferencing and presentation tool for e-learning, with the intent of eventually offering university courses online.  it's pretty fully featured software, and is meant to be installed as a server that can host many conference rooms at a time.  it has voice, video, text, and a whiteboard/slideshow (edit: supports pdf at the moment) capability.  one feature i think it neat is that students can 'raise their hands' in the class to ask the instructor a question, where they can take the floor for a moment.  check out the demo on the site (if it's not working anymore i'll nudge the developers).  another pro is that the clients only need to have flash installed.  i just logged onto the online demo and created this preview:  this project is now called bigbluebutton : http://code.google.com/p/bigbluebutton/  here is the demo: http://demo.bigbluebutton.org/ ",product
11,"tinymce only goes out of its way to disable spell-checking when you don't specify the gecko_spellcheck option (i verified this with their example code). might want to double-check your tinymce.init() call - it should look something like this:  tinymce.init({  mode : ""textareas"",  theme : ""simple"",  gecko_spellcheck : true });  ",javascript spell-checking
12,"programatically or manually   manually, i prefer adexplorer, which is a nice active directory browser. you just connect to your domain controller and then you can look for the user and see all the details. of course, you need permissions on the domain controller, not sure which though.  programatically, it depends on your language of couse. on .net, the system.directoryservices namespace is your friend. (i don't have any code examples here unfortunately)  for active directory, i'm not really an expert apart from how to query it, but here are two links i found useful:  http://www.computerperformance.co.uk/logon/ldap_attributes_active_directory.htm  http://en.wikipedia.org/wiki/active_directory (general stuff about the structure of ad) ",windows active-directory groups
13,"let me be general [then specific]:   your ide of choice [vs 2008 here] your debugger [it is usually part of your ide, but sometimes windbg is needed] its plugins for refactoring and source control [resharper 4+ and ankh svn 2+] your os's addons for source control [tortoise svn] a better diff and merge tool to plug into the above scm tools [winmerge] a fast loading text editor for when your ide is too much [vim, notepad++] if you're doing web development, get tools for that [firefox 3 with add-ons: web developer, firebug, tamperdata, poster, firecookie, fireftp, firephp, rainbow for firebug, reloadevery, selenium ide] requisite tools for working with text [gnu textutils, via cygwin or gnuwin32.sf.net] scripting tools [perl, python, zsh, all those gnu base packages in cygwin] a regular expression testing tool for when your eyes hurt [expresso, regexbuddy]   for java i swap out 1 and 3 with eclipse, and its plugins for maven and svn, i haven't found a refactoring plug in... you'd think i'd use intellij idea but i never started using it. ",windows-vista
14,"there are a few ambiguities in your question. what operation needs to be successful   for everything you want to know about drag and drop, browse through these search results (multiple pages worth):  raymond chen on drag and drop ",c# c++ windows winapi com
15,"anything but visual source safe; preferably one which supports the concepts of branching and merging. as others have said, subversion is a great choice, especially with the tortoisesvn client.  be sure to check out (pardon the pun) eric sink's classic series of source control howto articles. ",version-control
16,"not sure what the confusion is. sites on your intranet are in the intranet zone, web sites are in the internet zone, and sites on your computer are in the local zone, unless you've specifically overridden something in the browser's preferences. ",security internet-explorer-7 security-zone
17,"i don't mean this to sound snarky, but it doesn't matter.  seriously.  look at the things that are important: your project, your code, your job, your personal life. none of them are going to have their success rest on whether or not you use the ""this"" keyword to qualify access to fields. the this keyword will not help you ship on time. it's not going to reduce bugs, it's not going to have any appreciable effect on code quality or maintainability. it's not going to get you a raise, or allow you to spend less time at the office.   it's really just a style issue. if you like ""this"", then use it. if you don't, then don't. if you need it to get correct semantics then use it. the truth is, every programmer has his own unique programing style. that style reflects that particular programmer's notions of what the ""most aesthetically pleasing code"" should look like. by definition, any other programmer who reads your code is going to have a different programing style. that means there is always going to be something you did that the other guy doesn't like, or would have done differently. at some point some guy is going to read your code and grumble about something.   i wouldn't fret over it. i would just make sure the code is as aesthetically pleasing as possible according to your own tastes. if you ask 10 programmers how to format code, you are going to get about 15 different opinions. a better thing to focus on is how the code is factored. are things abstracted right  did i pick meaningful names for things  is there a lot of code duplication  are there ways i can simplify stuff  getting those things right, i think, will have the greatest positive impact on your project, your code, your job, and your life. coincidentally, it will probably also cause the other guy to grumble the least. if your code works, is easy to read, and is well factored, the other guy isn't going to be scrutinizing how you initialize fields. he's just going to use your code, marvel at it's greatness, and then move on to something else. ",c# coding-style
18,"@jeff  i actually think this is an interesting question. i'm not sure how useful it is, but it is a valid question.  @ed  can you provide a little more info on this question  you said the dimension of the array is dynamic, but is the number of elements dynamic as well   edit: i'm going to try and answer the question anyways. i can't give you the code off the top of my head (it would take a while to get it right without any compiler here on this pc), but i can point you in the right direction ...  let's use 8 dimensions (0-7) with indexes 0 to 3 as an example. you care about only 1,2 and 6. this means you have two arrays. first, array_care[4][4][4] for 1,2, and 6. the array_care[4][4][4] will hold the end result.  next, we want to iterate in a very specific way. we have the array input[4][4][4][4][4][4][4][4] to parse through, and we care about dimensions 1, 2, and 6.  we need to define some temporary indexes:  int dim[8] = {0,0,0,0,0,0,0,0};   we also need to store the order in which we want to increase the indexes:  int increase_index_order[8] = {7,5,4,3,0,6,2,1}; int i = 0;   this order is important for doing what you requested.  define a termination flag:  bool terminate=false;   now we can create our loop:  while (terminate) { array_care[dim[1]][dim[2]][dim[6]] += input[dim[0]][dim[1]][dim[2]][dim[3]][dim[4]][dim[5]][dim[6]][dim[7]];  while ((dim[increase_index_order[i]] = 3) &amp;&amp; (i &lt; 8)) { dim[increase_index_order[i]]=0; i++; }  if (i &lt; 8) { dim[increase_index_order[i]]++; i=0; } else { terminate=true; } }   that should work for 8 dimensions, caring about 3 dimensions. it would take a bit more time to make it dynamic, and i don't have the time. hope this helps. i apologize, but i haven't learned the code markups yet. :( ",c++ c arrays microsoft-dynamics
19,"the following will work  &lt;%= post.find_unread_by(current_user).size %&gt;   or  &lt;%= post.find_unread_by(current_user).length %&gt;   however if you check your development.log you should see that it gets the unread count by    retrieving all the posts retrieving all the posts read by the user removing all of 2. from 1. in ruby   this will be very bad performance wise with lots of posts.   a better way would be to retrieve the posts read by the current user and then use activerecord::calculations to get a count without retrieving all the posts in the database  post.count(:conditions =&gt; [ ""id not in ( )"", post.find_read_by(current_user)])   this should go into your post model to follow best practices of not having finders in the view  or controller   post.rb  def self.unread_post_count_for_user(user)   count(:conditions =&gt; [ ""id not in ( )"", post.find_read_by(user)]) end   then your view will just be  &lt;%= post.unread_post_count_for_user(current-user) %&gt;  ",ruby-on-rails ruby plugins
20,"encrypting the session value will have zero effect. the session cookie is already an arbitrary value, encrypting it will just generate another arbitrary value that can be sniffed.  the only real solution is https. if you don't want to do ssl on your whole site (maybe you have performance concerns), you might be able to get away with only ssl protecting the sensitive areas. to do that, first make sure your login page is https. when a user logs in, set a secure cookie (meaning the browser will only transmit it over an ssl link) in addition to the regular session cookie. then, when a user visits one of your ""sensitive"" areas, redirect them to https, and check for the presence of that secure cookie. a real user will have it, a session hijacker will not.  edit: this answer was originally written in 2008. it's 2016 now, and there's no reason not to have ssl across your entire site. no more plaintext http  ",security session cookies
21,set a short timeout   does isoutputshutdown() not get you what you want   you could always build a socketwatcher class that spins up in its own thread and repeatedly tries to write empty strings to the socket until that raises a socketclosedexception. ,java exception sockets networking
22,"in our scenario, we have a separate file server that both of our front end app servers write to, that way you either server has access to the same sets of files. ",asp.net iis-6 windows-server-2003 cluster-computing failover
23,"this is much more concise:  where    datediff(day, date1, date2) = 0  ",sql sql-server tsql datetime user-defined-functions
24,"there may be some confusion here about what ""k&amp;r c"" is. the term refers to the language as documented in the first edition of ""the c programming language."" roughly speaking: the input language of the bell labs c compiler circa 1978.  kernighan and ritchie were involved in the ansi standardization process. the ""ansi c"" dialect superceded ""k&amp;r c"" and subsequent editions of ""the c programming language"" adopt the ansi conventions. ""k&amp;r c"" is a ""dead language,"" except to the extent that some compilers still accept legacy code. ",c kr-c c89
25,"in most firewall setups, the tcp connection will be torn down by the firewall if it is idle to conserve resources. the idle timeout is probably not something you can control. some will tear them down if they are idle and a resource limit is being hit.  most corp environments won't allow any machines to make an outbound tcp connection anyway.  also, using this mechanism means you are going to have scaling problems. i think more reliable solution is to queue up information and have your clients poll for them regularly. utilize caching if possible such that a subsequent client poll will get the cached data from the customers proxy cache, if they are using one.  if you have to push data in a timely manner, in sub-second land (i.e. financial services), then consider some messaging infrastructure such an nservicebus distributor on client side, but that will require a customer install...  so have you tried using toredo  having read that it would appear there it is prob too complicated for a user to setup. ",.net wcf firewall push duplex
26,"i think the way to do(get around) this problem is to develop a ""./configure &amp;&amp; make"" script that your client uses to install, specify and compile the binaries. that would offcourse require that the client has all the source-code installed on his server or you can make it available on an nfs share. ",apache unix httpserver
27,i think you need to use display: block on your images. when images are inline there's a little extra space for the line spacing. ,html css
28,"bear in mind that the entlib documentation specifically steers you towards the asp.net cache for asp.net applications. that's probably the strongest recommendation towards using it here. plus the entlib cache doesn't have dependencies, which for me is a big reason not to use it.  i don't think there's a technical limitation as such on shipping system.web as part of your app, though it's slightly odd that they've put that notice in on the .net 3.5 page. hanselman actually says he started out being creeped out by this notion, but became convinced. also if you read the comments, he says that the block has too many moving parts and the asp.net cache is much more lightweght. i think this is exactly the kind of problem that velocity is going to solve, but that's only a preview for now :-(    i'd say use web.caching and see how you get on. if you put some kind of abstraction layer over the top of it, you've always got the option to swap it out for the entlib block later on if you find problems. ",caching .net-2.0 memcached enterprise-library
29,"the following (c#) code should do the trick  uri uri = new uri(""http://www.mywebsite.com:80/pages/page1.aspx""); string requested = uri.scheme + uri.schemedelimiter + uri.host + "":"" + uri.port;  ",.net asp.net url uri authority
30,"i finally got multisampling working with my wxwidgets opengl program. it's a bit messy right now, but here's how:  wxwidgets doesn't have multisampling support in their stable releases right now (latest version at this time is 2.8.8). but, it's available as a patch and also through their daily snapshot. (the latter is heartening, since it means that the patch has been accepted and should appear in later stable releases if there are no issues.)  so, there are 2 options:   download and build from their daily snapshot. get the patch for your working wxwidgets installation.   i found the 2nd option to be less cumbersome, since i don't want to disturb my working installation as much as possible. if you don't know how to patch on windows, see this.  at the very least, for windows, the patch will modify the following files:  $(wx_widgets_root)/include/wx/glcanvas.h $(wx_widgets_root)/include/wx/msw/glcanvas.h $(wx_widgets_root)/src/msw/glcanvas.cpp   after patching, recompile the wxwidgets libraries.  to enable multisampling in your wxwidgets opengl program, minor changes to the code are required.  an attribute list needs to be passed to the wxglcanvas constructor:  int attriblist[] = {wx_gl_rgba,                     wx_gl_doublebuffer,                     wx_gl_sample_buffers, gl_true, // multi-sampling                     wx_gl_depth_size, 16,                     0, 0};   if you were already using an attribute list, then add the line with gl_sample_buffers, gl_true to it. else, add this attribute list definition to your code.  then modify your wxglcanvas constructor to take this attribute list as a parameter:  myglframe::myglframe    // derived from wxglcanvas (     wxwindow *parent,     wxwindowid id,     const wxpoint&amp; pos,     const wxsize&amp; size,     long style,     const wxstring&amp; name ) : wxglcanvas(parent, (wxglcanvas*) null, id, pos, size, style, name, attriblist) {     // ... }   after the wxglcanvas element is created, multisampling is turned on by default. to disable or enable it at will, use the related opengl calls:  glenable(gl_multisample); gldisable(gl_multisample);   multisampling should now work with the wxwidgets opengl program. hopefully, it should be supported in the stable release of wxwidgets soon, making this information irrelevant :-) ",opengl wxwidgets multisampling
31,"well, if it support python, you might be able to run django using a fastcgi setup, as described here: fastcgi docs. if it supports mod_python (which i guess it's what you mean), then sure, you can install it using the steps listed here: modpython docs ",django shared-hosting hostmonster
32,"linq is not about sql. linq is about being apply functional programming paradigmns on objects.  linq to sql is an orm built ontop of the linq foundation, but linq is much more. i don't use linq to sql, yet i use linq all the time.  take the task of finding the intersection of two lists:  before linq, this tasks requires writing a nested foreach that iterates the small list once for every item in the big list o(n*m), and takes about 10 lines of code.  foreach (int number in list1) {     foreach (int number2 in list2)     {         if (number2 == number)         {             returnlist.add(number2);         }     } }   using linq, it does the same thing in one line of code:  var results = list1.intersect(list2);   you'll notice that doesn't look like linq, yet it is. you don't need to use the expression syntax if you don't want to. ",c# sql linq
33,not that i know of. this has been a plague of windows versions for quite some time. ,windows configuration
34,"i think you should look at silverlight as a long-term play, just as microsoft seems to be doing. there's an obvious balance on when to use silverlight vs. flash when you're concerned about reach and install base, but here are some reasons silverlight is a good direction to move in:   second mover advantage - just as microsoft built a ""better java"" with .net, they're able to look at how you'd design a ria plugin from scratch, today. they have the advantage of knowing how people use the web today, something the inventors of flash could never have accurately guessed. flash can add features, but they can't realistically chuck the platform and start over. developer familiarity - while silverlight is a new model, it's not entirely unfamiliar to developers. they'll ""get"" the way silverlight works a lot more quickly than they'll understand firing up a new development environment with a new scripting language and new event paradigms. being rid of the timeline model in flash - flash was originally built for keyframe based animations, and while there are ways to abstract this away, it's at the core of how flash works. silverlight ditches that for an application-centric model.  scottgu - scottgu is fired up about silverlight. nuff said. cool new features - while silverlight still has some catching up to do with flash on some obvious features (like webcam / mic integration, or 3d / graphics acceleration), there are some slick new technologies built in to silverlight - deep zoom is one example. i'm seeing more ""revolutionary"" technologies on the silverlight side, while flash seems to be in maintenance mode at this point.  ",.net flex flash silverlight
35,"can you give us some more information on how you add the combo boxes to the jpanel   this is a pretty common thing to do in swing so i doubt that it's a jvm issue but i guess anything is possible.  specifically, i would double check to make sure you're not accessing the gui from any background threads.  in this case, maybe you're reading the choices from a db or something and updating the jcombobox from a background thread, which is a big no-no in swing.  see swingutils.invokelater(). ",java swing jpanel
36,"this is not directly an answer, but     the thing is that to remove the disk you have to eject the volume and in this case do it for both volumes   i have a similar situation.  osx remembers where you put your icons on the desktop - i've moved the icons for both of my removable drives to just above where the trash can lives.  eject procedure becomes   hit top-left of screen with mouse to show desktop drag small box around both removable drives drag 2cm onto trash so they both get ejected remove firewire cable  ",osx hardware
37,"from msdn:     when this option is set, it is possible to read uncommitted or dirty data; values in the data can be changed and rows can appear or disappear in the data set before the end of the transaction.    simply put, when you are using this isolation level, and you are performing multiple queries on an active table as part of one transaction, there is no guarantee that the information returned to you within different parts of the transaction will remain the same. you could query the same data twice within one transaction and get different results (this might happen in the case where a different user was updating the same data in the midst of your transaction). this can obviously have severe ramifications for parts of your application that rely on data integrity. ",sql sql-server
38,"there's no need for the javascript function, the first argument to externalinterface can be any javascript code, it doesn't have to be a function name (the documentation says so, but it is wrong).  try this:  externalinterface.call(""alert('hello')"");  ",javascript flex exploit
39,"i would use a hidden richtextbox, set the rtf member, then retrieve the text member to sanitize the rtf in a well-supported way.  then i would use manually inject the desired formatting afterwards. ",.net vb.net ms-word rtf
40,"you sound as if you're looking for three major things: code templates, refactoring tools, and auto-completion.  the good news is that xcode 3 and later come with superb auto-completion and template support.  by default, you have to explicitly request completion by hitting the escape key.  (this actually works in all nstextviews; try it )  if you want to have the completions appear automatically, you can go to preferences -> code sense and set the pop-up to appear automatically after a few seconds.  you should find good completions for c and objective-c code, and pretty good completions for c++.  xcode also has a solid template/skeleton system that you can use.  you can see what templates are available by default by going to edit -> insert text macro.  of course, you don't want to insert text macros with the mouse; that defeats the point.  instead, you have two options:   back in preferences,go to key bindings, and then, under menu key bindings, assign a specific shortcut to macros you use often.  i personally don't bother doing this, but i know plenty of great mac devs who do use the completionprefix.  by default, nearly all of the templates have a special prefix that, if you type and then hit the escape key, will result in the template being inserted.  you can use control-/ to move between the completion fields.  you can see a full list of xcode's default macros and their associated completionprefixes at crooked spin.  you can also add your own macros, or modify the defaults.  to do so, edit the file /developer/library/xcode/specifications/{c,html}.xctxtmacro.  the syntax should be self-explanatory, if not terribly friendly.   unfortunately, if you're addicted to r#, you will be disappointed by your refactoring options.  basic refactoring is provided within xcode through the context menu or by hitting shift-apple-j.  from there, you can extract and rename methods, promote and demote them through the class hierarchy, and a few other common operations.  unfortunately, neither xcode nor any third-party utilities offer anything approaching resharper, so on that front, you're currently out of luck.  thankfully, apple has already demonstrated versions of xcode in the works that have vastly improved refactoring capabilities, so hopefully you won't have to wait too long before the situation starts to improve. ",objective-c xcode osx refactoring
41,"the number one rule is do not just ask for status updates.  it is especially annoying when phrases like ""where are we on this "" are used.   if you aren't directly involved in the details then just make sure you have established communication times or plans so that you know whats going on rather than asking for updates. ",.net project-management
42,"i don't really know javascript, but couldn't you create a stack of windows  ",javascript
43,"you can use a datalist control instead. it has a repeatcolumns property that you can define the number of columns you want to display.  in .net framework 3.5, there is an even better solution, the listview control. you can find further information about how to use the listview control here. ",asp.net gridview infragistics ultrawebgrid
44,"javascript offers 3 modal boxes.  prompt, confirm and alert.  none of those satisfy your request.    there are a plethora of js modal popup solutions.  here's an example.   modalbox  ",javascript
45,"   i want to add a rule that checks for   the presence of a folder.jpg file in   each directory, but to add this would   make the code substantially more messy   in it's current state..   this doesn't look bad.  in fact your current code does it very nicely, and sven mentioned a good way to do it as well:   get a list of all the files check for ""required"" files   you would just have have add to your dictionary a list of required files:  checker = {   ...   'required': ['file', 'list', 'for_required'] }   as far as there being a better/extensible way to do this   i am not exactly sure.  i could only really think of a way to possibly drop the ""multiple"" regular expressions and build off of sven's idea for using a delimiter.  so my strategy would be defining a dictionary as follows (and i'm sorry i don't know python syntax and i'm a tad to lazy to look it up but it should make sense.  the /regex/ is shorthand for a regex):  check_dict = {   'delim'    : /\-/,   'parts'    : [ 'show name', 'episode name', 'episode number' ],   'patterns' : [/valid name/, /valid episode name/, /valid number/ ],   'required' : ['list', 'of', 'files'],   'ignored'  : ['.*', 'hidden.txt'],   'start_dir': '/path/to/dir/to/test/' }    split the filename based on the delimiter. check each of the parts.   because its an ordered list you can determine what parts are missing and if a section doesn't match any pattern it is malformed.  here the parts and patterns have a 1 to 1 ratio.  two arrays instead of a dictionary enforces the order.  ignored and required files can be listed.  the . and .. files should probably be ignored automatically.  the user should be allowed to input ""globs"" which can be shell expanded.  i'm thinking here of svn:ignore properties, but globbing is natural for listing files.  here start_dir would be default to the current directory but if you wanted a single file to run automated testing of a bunch of directories this would be useful.  the real loose end here is the path template and along the same lines what path is required for ""valid files"".  i really couldn't come up with a solid idea without writing one large regular expression and taking groups from it... to build a template.  it felt a lot like writing a textmate language grammar.  but that starts to stray on the ease of use.  the real problem was that the path template was not composed of parts, which makes sense but adds complexity.  is this strategy in tune with what you were thinking of  ",python validation naming
46,"it depends on what browser you use, but internet explorer, for example, would take you to the root directory of your harddrive (eg. c:/), while browsers such as firefox does nothing.  ",html css directory
47, popularity community contribution public scrutiny we will be forced to adhere to standards. (which will in turn make the product better) goodwill  ,open-source licensing
48,"i've been using forms authentication and creating the necessary genericidentity and customprincipal objects that allows me to leverage the user.isinrole type functions you typically only get with windows authentication.  that way in my web.config file, i can do stuff like...  &lt;location path=""login.aspx""&gt;    &lt;system.web&gt;       &lt;authorization&gt;          &lt;allow users =""*"" /&gt;       &lt;/authorization&gt;    &lt;/system.web&gt; &lt;/location&gt;  &lt;location path=""managementfolder""&gt;    &lt;system.web&gt;       &lt;authorization&gt;          &lt;allow roles =""administrator, manager"" /&gt;       &lt;/authorization&gt;    &lt;/system.web&gt; &lt;/location&gt;  ",asp.net authorization
49,"it has pretty extensive coverage up to .net 4.0 and even include some features from .net 4.5 apis, but there are a few areas that we have chosen not to implement due to the apis being deprecated, new alternatives being created or the scope being too large.   the following apis are not available in mono:   windows presentation foundation windows workflow foundation (neither of the two versions) entity framework the wse1/wse2 ""add-ons"" to the standard web services stack   additionally, our wcf implementation is limited to what silverlight supported.  the easiest way to check for your specific project is to run the mono migration analyzer (moma). the benefit is that it will notify the mono team of issues which will prevent you from using mono (if any), which lets them prioritize their work.  i recently ran moma on subsonic and found only one issue - a weird use of nullable types. that's a big codebase, so the coverage there was pretty impressive.  mono is in active use in several commercial as well as open source products. it's in use in some large applications, such as wikipedia and the mozilla developer center, and has been used in embedded applications such as the sansa mp3 players and powers thousands of published games.  at the language level, the mono compiler is fully compliant with the c# 5.0 language specification. ",.net open-source mono
50,"you'll also want to make sure that your ssh system is well secured to prevent people from logging in as your capistrano bot.  i'd suggest restricting access to password-protected key pairs.  encrypting the .yml file on the server is useless since you have to give the bot the key, which would be stored . . . on the same server.  encrypting it on your machine is probably a good idea.  capistrano can decrypt it before sending. ",ruby-on-rails security deployment
51,"delete the executable as part of a pre-link event.  edit: hah, i forgot about explorer resetting the creation date if you name a file exactly the same as a file that was recently deleted.  why are you keying off the creation date anyway  ",date executable
52,"if you're after things like server data, would rrdtool be something you're after  it's not really a webserver type stats program though, i have no idea how it would scale.  edit:  i've also just found splunk swarm, if you're interested in something that looks ""cool"". ",performance monitoring usage-statistics
53,"older ankhsvn (pre 2.0) was very crappy and i was only using it for shiny icons in the solution explorer. i relied on tortoise for everything except reverts.  the newer ankh is a complete rewrite (it is now using the source control api of the ide) and looks &amp; works much better. still, i haven't forced it to any heavy lifting. icons is enough for me.  the only gripe i have with 2.0 is the fact that it slaps its footprint to .sln files. i always revert them lest they cause problems for co-workers who do not have ankh installed. dunno if my fears are groundless or not.    addendum:  i have been using v2.1.7141 a bit more extensively for the last few weeks and here are the new things i have to add:   no ugly crashes that plagued v1.x. yay  for some reason, ""show changes"" (diff) windows are limited to only two. meh. diff windows do not allow editing/reverting yet. boo  updates, commits and browsing are much faster than tortoise. yay    all in all, i would not use it standalone, but once you start using it, it becomes an almost indispensable companion to tortoise. ",version-control ankhsvn
54,"http://en.wikipedia.org/wiki/printf#printf_format_placeholders is wikipedia's reference for format placeholders in printf. http://www.cplusplus.com/reference/clibrary/cstdio/printf.html is also helpful  basically in a simple form it's %[width].[precision][type]. width allows you to make sure that the variable which is being printed is at least a certain length (useful for tables etc). precision allows you to specify the precision a number is printed to (eg. decimal places etc) and the informs c/c++ what the variable you've given it is (character, integer, double etc).  hope this helps  update:  to clarify using your examples:  printf( ""%10.1f     %10.2\n"", radius, area );   %10.1f (referring to the first argument: radius) means make it 10 characters long (ie. pad with spaces), and print it as a float with one decimal place.  %10.2 (referring to the second argument: area) means make it 10 character long (as above) and print with two decimal places. ",c
55,"i'm guessing that the settings would have to be in powershell.exe.config in the powershell directory, but that seems to be a bad way of doing things.  you can use configurationmanager.openmappedexeconfiguration to open a configuration file based on the executing dll name, rather than the application exe, but this would obviously require changes to the dlls. ",powershell configuration-files
56,"i'd suggest that modifying the underlying xml file is ""considered harmful"".  especially if you haven't checked to see if the document is open   i've had a quick look at the scripting dictionary for pages, and it seems pretty comprehensive; here is part of one entry:     documentân [inh. document > item; see also standard suite] : a pages document.      elements      contains captured pages, character   styles, charts, graphics, images,   lines, list styles, pages, paragraph   styles, sections, shapes, tables, text   boxes.      properties      body text (text) : the main text flow of the document.      bottom margin (real) : the bottom margin of the publication.      facing pages (boolean) : whether or not the view is set to facing   pages.      footer margin (real) : the footer margin of the publication.      header margin (real) : the header margin of the publication.      id (integer, r/o) : the unique identifier of the document.      ...   so, i guess i'd want to know what it is that you want to do that you can't do with applescript  ",xml automation applescript iwork
57,"sure thing, the simplest way is this:  select foo from bar where baz in (1,2,3)  ",mysql
58,i found an interesting article about implementing a parser combinator in c#. it also references some more general papers on the subject.  the wikipedia article on the subject also has a general explaination of the concept.  ,parsing monads
59,two things i do:   i blog about it - this allows me to go back and search my own blog. we use the code snippet feature in visual studio.   cheers. ,knowledge-management
60,any semi-decent algorithm will end up with a strong chance of generating a null value somewhere in the resulting ciphertext.  why not do something like base-64 encode your resulting binary blob before persisting to the db  (sample implementation in c++). ,c++ encryption winapi
61,"i can't find any documentation that says you shouldn't use those methods directly, but i haven't looked very long.  also you refer to the editorvisibleattribute, which doesn't exist. according to reflector it's the editorbrowsableattribute.  reflector disassembly:  [editorbrowsable(editorbrowsablestate.never)] public bool checkaccess() { //code }  ",.net-3.5 intellisense
62,"you don't test the converter, you test the final code.   if the code doesn't compile, clearly your converter is failing. if the code compiles and your functionality tests fail, then you can tweak the code so that it passes the test. if you are fairly successful you should see that you only need to fix the modules that actually fail.  goodluck  ",c# testing
63,"yes, the opengl extension wrangler library (glew) is a painless way to use opengl extensions on windows. here's how to get started on it:  identify the opengl extension and the extension apis you wish to use. opengl extensions are listed in the opengl extension registry.  check if your graphic card supports the extensions you wish to use. download and install the latest drivers and sdks for your graphics card.  recent versions of nvidia opengl sdk ship with glew. if you're using this, then you don't need to do some of the following steps.  download glew and unzip it.  add the glew bin path to your windows path environment variable. alternatively, you can also place the glew32.dll in a directory where windows picks up its dlls.  add the glew include path to your compiler's include directory list.  add the glew lib path to your compiler's library directory list.  instruct your compiler to use glew32.lib during linking. if you're using visual c++ compilers then one way to do this is by adding the following line to your code:  #pragma comment(lib, ""glew32.lib"")   add a #include &lt;gl/glew.h&gt; line to your code. ensure that this is placed above the includes of other gl header files. (you may actually not need the gl header files includes if you include glew.h.)  initialize glew using glewinit() after you've initialized glut or gl. if it fails, then something is wrong with your setup.  if (glew_ok  = glewinit()) {     // glew failed      exit(1); }   check if the extension(s) you wish to use are now available through glew. you do this by checking a boolean variable named glew_your_extension_name which is exposed by glew.     example:   if ( glew_ext_framebuffer_object) {     exit(1); }   that's it  you can now use the opengl extension calls in your code just as if they existed naturally for windows. ",opengl glew
64,"ben, almost all seasoned programmers are still students in programming. you never stops learning anything when you are a developer. but if you are really starting off on your career then you should be least worried about the specialization thing. all apis, frameworks and skills that you expect that gives you a long term existence in the field is not going to happen. technology seems changing a lot and you should be versatile and flexible enough to learn anything. the knowledge you acquire on one platform/api/framework doesn't die off. you can apply the skills to the next greatest platform/api/framework.   that being said you should just stop worrying about the future and concentrate on the basics. datastructures, algorithm analysis and design, compiler design, operating system design are the bare minimum stuff you need.  and further you should be willing to go back and read tho books in those field any time in your career. thats all is required. good luck.   sorry if i sounded like a big ass advisor; but thats what i think. :-) ",language-agnostic
65,"you can use the chilkat library.  it's commercial, but has a free evaluation and seems pretty nice.  here's an example i got from here:  import chilkat  # demonstrates how to create a winzip-compatible 128-bit aes strong encrypted zip zip = chilkat.ckzip() zip.unlockcomponent(""anything for 30-day trial"")  zip.newzip(""strongencrypted.zip"")  # set the encryption property = 4, which indicates winzip compatible aes encryption. zip.put_encryption(4) # the key length can be 128, 192, or 256. zip.put_encryptkeylength(128) zip.setpassword(""secret"")  zip.appendfiles(""exampledata/*"",true) zip.writezip()  ",python zip
66,"there are two rules i follow:   accept the most basic type that will work return the richest type your user will need   so when writing a function or method that takes a collection, write it not to take a list, but an ilist&lt;t&gt;, an icollection&lt;t&gt;, or ienumerable&lt;t&gt;.  the generic interfaces will still work even for heterogenous lists because system.object can be a t too.  doing this will save you headache if you decide to use a stack or some other data structure further down the road.  if all you need to do in the function is foreach through it, ienumerable&lt;t&gt; is really all you should be asking for.  on the other hand, when returning an object out of a function, you want to give the user the richest possible set of operations without them having to cast around.  so in that case, if it's a list&lt;t&gt; internally, return a copy as a list&lt;t&gt;. ",c# .net
67,"if you want to perform more complicated operations, like reading the output of the external program, you may be better served by the popen system call. for example, to programmatically access a directory listing (this is a somewhat silly example, but useful as an example), you could write something like this:  #include &lt;stdio.h&gt;  int main() {   int entry = 1;   char line[200];   file* output = popen(""/usr/bin/ls -1 /usr/man"", ""r"");   while ( fgets(line, 199, output) )   {     printf(""%5d: %s"", entry++, line);   } }   to give output like this  1: cat1 2: cat1b 3: cat1c 4: cat1f 5: cat1m 6: cat1s ...  ",c process
68,if you partition your drive using lvm you won't have to worry about any individual partition running out of space in the future. just move space around as necessary. ,linux storage disk-partitioning
69,"none of the apis for the more 'word' like online editors seem to have any 'edit' functionality, just download and upload. this is obviously due to the complexity of the documents being stored.  with google docs api you can get a document, edit it, and then re-upload it (you may need to delete the previous version as well i think). zoho seems to provide an 'update' method which combines the two operations.  have you looked at the google notebook api instead  this might provide more granular access to the data in the 'note' due to its structured approach to storing the data. ",api
70,you can right click on the tab strip and insert a new vertical (or horizontal) tab group.  this allows you to view multiple tabs at the same time. ,visual-studio-2008
71,ok here's some best practices from some one who doesn't unit test as much as he should...cough.   make sure your tests test one thing and one thing only. write unit tests as you go. preferably before you write the code you are testing against. do not unit test the gui.  separate your concerns.  minimise the dependencies of your tests. mock behviour with mocks.  ,unit-testing
72,"""algebraic data types"" in haskell support full parametric polymorphism, which is the more technically correct name for generics, as a simple example the list data type:   data list a = cons a (list a) | nil   is equivalent (as much as is possible, and ignoring non-strict evaluation, etc) to   class list&lt;a&gt; {      class cons : list&lt;a&gt; {          a head;          list&lt;a&gt; tail;      }      class nil : list&lt;a&gt; {}  }   of course haskell's type system allows more ... interesting use of type parameters but this is just a simple example.  with regards to the ""algebraic type"" name, i've honestly never been entirely sure of the exact reason for them being named that, but have assumed that it's due the mathematical underpinnings of the type system.  i believe that the reason boils down to the theoretical definition of an adt being the ""product of a set of constructors"", however it's been a couple of years since i escaped university so i can no longer remember the specifics.  [edit: thanks to chris conway for pointing out my foolish error, adt are of course sum types, the constructors providing the product/tuple of fields] ",data-structures haskell types functional-programming algebraic-data-types
73,"as far as i know, there is no way to localize a report (meaning automating the translation of string litterals)...  like you said,you basically have to use the user language global variable to catch the user's settings and then use that to retrieve the appropriate strings from the db...  however, you can adapt the display of currency/numeric/date fields according to the user locale. also possible is changing the interface of the report viewer to match your user's langage.  here are a few links giving tips on how to adapt the locale:  http://www.ssw.com.au/ssw/standards/rules/rulestobettersqlreportingservices.aspx#languagesetting  langage pack for report viewer:  http://www.microsoft.com/downloads/details.aspx displaylang=en&amp;familyid=e3d3071b-d919-4ff9-9696-c11d312a36a0 ",reporting-services internationalization
74,you should also check hardware connectivity to the database.  perhaps this thread will be helpful: http://channel9.msdn.com/forums/techoff/234271-conenction-forcibly-closed-sql-2005/ ,c# sql-server tsql ado.net sql-server-2000
75,"we actually use a combination of nant and msbuild with cruisecontrol. nant is used for script flow control and calls msbuild to compile projects. after the physical build is triggered, nant is used to publish the individual project build outputs to a shared location.  i am not sure this is the best process. i think many of us are still looking for a great build tool. one promising thing i heard recently on .net rocks, episode 362, is james kovac's psake, a build system he based entirely on powershell. it sounds really promising since what you can do with powershell is fairly limitless in theory. ",.net build-process nant
76,"if you want to specify both a limit for number of items to remove and a condition to select the items to remove, you can use this approach:  int limit = 30; // suppose you want to remove 30 items at most list.removeall(item =&gt; shouldiremovethis(item) &amp;&amp; limit-- &gt; 0);  ",.net
77,"the hash code of an object shouldn't be unique.   the checking rule is:    are the hash codes equal  then call the full (slow) equals method. are the hash codes not equal  then the two items are definitely not equal.   all you want is a gethashcode algorithm that splits up your collection into roughly even groups - it shouldn't form the key as the hashtable or dictionary&lt;&gt; will need to use the hash to optimise retrieval.  how long do you expect the data to be  how random  if lengths vary greatly (say for files) then just return the length.  if lengths are likely to be similar look at a subset of the bytes that varies.  gethashcode should be a lot quicker than equals, but doesn't need to be unique.  two identical things must never have different hash codes. two different objects should not have the same hash code, but some collisions are to be expected (after all, there are more permutations than possible 32 bit integers). ",c# hash
78,"there are a couple ways, create an action for each step of the wizard process, or create a parameter that is passed in to the action method.  like step that will allow you to know what the state of the wizard is in. ",asp.net-mvc
79,"i like a single object to be correlated to a single record in the database, i.e. an object must be instantiated.  this is your basic activerecord pattern.  in my experience, the one-object-to-one-row approach creates a much more fluid and literate presentation in code.  also, i like to treat objects as records and the class as the table. for example to change the name of a record i do:  objperson = new person(id)  objperson.name = ""george""  objperson.save()   while to get all people who live in louisiana i might do  arypeople = person::getpeoplefromstate(""la"")   there are plenty of criticisms of active record.  you can especially run into problems where you are querying the database for each record or your classes are tightly coupled to your database, creating inflexibility in both.  in that case you can move up a level and go with something like datamapper.   many of the modern frameworks and orm's are aware of some of these drawbacks and provide solutions for them.  do a little research and you will start to see that this is a problem that has a number of solutions and it all depend on your needs.  ",database orm class-design
80,"well, since you can not have one part osgi and one part non-osgi you'll need to make your entire app osgi. in its simplest form you make a single osgi bundle out of your entire application. clearly this is not a best practice but it can be useful to get a feel for deploying a bundle in an osgi container (equinox, felix, knoplerfish, etc).  to take it to the next level you'll want to start splitting your app into components, components should typically have a set of responsibilities that can be isolated from the rest of your application through a set of interfaces and class dependencies. identifying these purely by hand can range from rather straightforward for a well designed highly cohesive but loosely coupled application to a nightmare for interlocked source code that you are not familiar with.  some help can come from tools like jdepend which can show you the coupling of java packages against other packages/classes in your system. a package with low efferent coupling should be easier to extract into an osgi bundle than one with high efferent coupling. even more architectural insight can be had with pro tools like structure 101.  purely on a technical level, working daily with an application that consists of 160 osgi bundles and using spring dm i can confirm that the transition from ""normal"" spring to spring dm is largely pain free. the extra namespace and the fact that you can (and should) isolate your osgi specific spring configuration in separate files makes it even easier to have both with and without osgi deployment scenarios.  osgi is a deep and wide component model, documentation i recommend:   osgi r4 specification: get the pdfs of the core and compendium specification, they are canonical, authoritative and very readable. have a shortcut to them handy at all times, you will consult them. read up on osgi best practices, there is a large set of things you can do but a somewhat smaller set of things you should do and there are some things you should never do (dynamicimport: * for example).    some links:    osgi best practices and using apache felix peter kriens and bj hargrave in a sun presentation on osgi best practices  one key osgi concept are services, learn why and how they supplant the listener pattern with the whiteboard pattern the spring dm google group is very responsive and friendly in my experience the spring dm google group is no longer active and has moved to eclipse.org as the gemini blueprint project which has a forum here.  ",java spring osgi
81,"hook into the keypress event on the textbox, and when it encounters the enter key, run your hardware setting code, and then highlight the full text of the textbox again (see below) - windows will take care of clearing the text with the next keystroke for you.  textbox1.select(0, textbox1.text.length);  ",c# winforms textbox
82,"it's rather ugly:  statusenum mystatus = (statusenum) enum.parse(typeof(statusenum), ""active"", true);   i tend to simplify this with:  public static t parseenum&lt;t&gt;(string value) {     return (t) enum.parse(typeof(t), value, true); }   then i can do:  statusenum mystatus = enumutil.parseenum&lt;statusenum&gt;(""active"");   one option suggested in the comments is to add an extension, which is simple enough:  public static t toenum&lt;t&gt;(this string value) {     return (t) enum.parse(typeof(t), value, true); }  statusenum mystatus = ""active"".toenum&lt;statusenum&gt;();   finally, you may want to have a default enum to use if the string cannot be parsed:  public static t toenum&lt;t&gt;(this string value, t defaultvalue)  {     if (string.isnullorempty(value))     {         return defaultvalue;     }      t result;     return enum.tryparse&lt;t&gt;(value, true, out result)   result : defaultvalue; }   which makes this the call:  statusenum mystatus = ""active"".toenum(statusenum.none);   however, i would be careful adding an extension method like this to string as (without namespace control) it will appear on all instances of string whether they hold an enum or not (so 1234.tostring().toenum(statusenum.none) would be valid but nonsensical) . it's often be best to avoid cluttering microsoft's core classes with extra methods that only apply in very specific contexts unless your entire development team has a very good understanding of what those extensions do.  ",c# string enums
83,here are some 3rd-party diagramming tools:   http://www.nevron.com/products.diagramfor.net.overview.aspx http://www.nwoods.com/go/dotnet.htm http://www.syncfusion.com/products/diagram/web/default.aspx  ,asp.net asp.net-mvc .net-3.5 graphics diagram
84,"update: i have found a 3rd party com library called outlook redemption which is working fine for me at the moment. if you use it via com-interop in .net, don't forget to release every com object after you are done with it, otherwise your application crashes randomly. ",c# outlook email message msg
85,if you're using a winforms app you could try using userproperties to store this info. another possible solution could be custom configuration sections. ,.net enterprise-library connection-string
86,"do you know much about oop   if so, look into spring and hibernate to keep your implementation clean and orthogonal.  if you get that, you should find tdd a good way to keep your design compact and lean, especially since you have ""automated testing"" up and running.  update: looking at the first slew of answers, i couldn't disagree more.  particularly in the java space, you should find plenty of mentors/resources on working out your application with objects, not a database-centric approach.  database design is typically the first step for microsoft folks (which i do daily, but am in a recovery program, er, alt.net).  if you keep the focus on what you need to deliver to a customer and let your orm figure out how to persist your objects, your design should be better. ",java oop design architecture
87,"just change ""home"" to an empty string.  routes.maproute(     ""home"",     """",     new { action = index, controller = home } );  ",asp.net-mvc asp.net-mvc-routing
88,"the approach that i've found works for me is having seperate js files for each class (just as you would in java, c# and others). alternatively you can group your js into application functional areas if that's easier for you to navigate.  if you put all your js files into one directory, you can have your server-side environment (php for instance) loop through each file in that directory and output a &lt;script src='/path/to/js/$file.js' type='text/javascript'&gt; in some header file that is included by all your ui pages. you'll find this auto-loading especially handy if you're regularly creating and removing js files.  when deploying to production, you should have a script that combines them all into one js file and ""minifies"" it to keep the size down. ",javascript web-applications deployment
89,"      does anyone know of anyway to   reduce/optimize the pdf export phase   and or the size of the pdf without   lowering the total page count       i have a few ideas and questions: 1. is this a graphics-heavy report   if not, do you have tables that start out as text but are converted into a graphic by the ssrs pdf renderer (check if you can select the text in the pdf)   41k per page might be more than it should be, or it might not, depending on how information-dense your report is.  but we've had cases where we had minor issues with a report's layout, like having a table bleed into the page's margins, that resulted in the ssrs pdf renderer ""throwing up its hands"" and rendering the table as an image instead of as text.  obviously, the fewer graphics in your report, the smaller your file size will be. 2. is there a way that you could easily break the report into pieces   e.g., if it's a 10-location report, where location 1 is followed by location 2, etc., on your final report, could you run the location 1 portion independent of the location 2 portion, etc.   if so, you could join the 10 sub-reports into one final pdf using pdfsharp after you've received them all.  this leads to some difficulties with page numbering, but nothing insurmountable.     3. does anyone else have any other   theories as to why this runs on the   server but not through the api    my guess would be the sheer size of the report.  i don't remember everything about what's an iis setting and what's ssrs-specific, but there might be some overall iis settings (maybe in metabase.xml) that you would have to be updated to even allow that much data to pass through.   you could isolate the question of whether the time is the problem by taking one of your working reports and building in a long wait time in your stored procedures with waitfor (assuming sql server for your dbms).  not solutions, per se, but ideas.  hope it helps. ",sql-server reporting-services
90,"i still do it the old way, by defining a macro (xtrace, below) which correlates to either a no-op or a function call with a variable argument list. internally, call vsnprintf so you can keep the printf syntax:  #include &lt;stdio.h&gt;  void xtrace0(lpctstr lpsztext) {    ::outputdebugstring(lpsztext); }  void xtrace(lpctstr lpszformat, ...) {     va_list args;     va_start(args, lpszformat);     int nbuf;     tchar szbuffer[512]; // get rid of this hard-coded buffer     nbuf = _vsnprintf(szbuffer, 511, lpszformat, args);     ::outputdebugstring(szbuffer);     va_end(args); }   then a typical #ifdef switch:  #ifdef _debug #define xtrace xtrace #else #define xtrace #endif   well that can be cleaned up quite a bit but it's the basic idea. ",c++ c c-preprocessor
91,"i am not sure i completely understand your question.   if you are referring to the folder the ide has as the current folder, then you can just change the shortcut that launches delphi to set the current directory where ever you want it to be.  ",delphi ide
92,"assuming you have vmware workstation, vmware player or anything that can run vmware appliance, you just need to:   download, unzip ubuntu 8.04 server and start the virtual machine. update ubuntu and set the layout and the timezone:  sudo apt-get update sudo apt-get upgrade sudo dpkg-reconfigure console-setup sudo dpkg-reconfigure tzdata sudo vim /etc/network/interfaces  set a fixed ip (optional).  install apache+mysql+php: sudo tasksel install lamp-server   ",ubuntu vmware lamp
93,"in solutions with multiple projects, one thing i've found very helpful is to have all the assemblyinfo files point to a single project that governs the versioning. so my assemblyinfos have a line:  [assembly: assemblyversion(foo.staticversion.bar)]   i have a project with a single file that declares the string:  namespace foo {     public static class staticversion     {          public const string bar= ""3.0.216.0""; // 08/01/2008 17:28:35     } }   my automated build process then just changes that string by pulling the most recent version from the database and incrementing the second last number.  i only change the major build number when the featureset changes dramatically.  i don't change the file version at all. ",.net attributes versions
94,"this is a really cool feature. i've poked through the feature documentation, and the accompanying command list, and not a heck of a lot is showing up in terms of turning it off.  if you want to search for &gt;exit, you could always type &gt;edit.find &gt;exit in the search box; that seems to do the trick. a bit verbose, though, but it really is an edge case. ",visual-studio
95,"   should i just do it when i'm going to use it    i would recommend that you only retrieve the data when you are going to use it. if you are not going to need it, there is no reason to waste resources by retrieving it in page_load. if you are going to need it multiple times throughout the page load, consider saving the query results to a private variable or collection so that the same data can be reused multiple times throughout the page load.     am i opening a new connection when i create a new instance    asp.net handles connection pooling, and opens and closes connections in an efficient way. you shouldn't have to worry about this.  one other thing to consider from a performance perspective is to avoid using datasets and tableadapters. in many cases, they add extra overhead into data retrieval that does not exist when using linq to sql, stored procedures or datareaders. ",asp.net database performance
96,"i am generally a proponent of putting everything in stored procedures, for all of the reasons dbas have been harping on for years.  in the case of linq, it is true that there will be no performance difference with simple crud queries.  but keep a few things in mind when making this decision: using any orm couples you tightly to your data model.  a dba has no freedom to make changes to the data model without forcing you to change your compiled code.  with stored procedures, you can hide these sorts of changes to an extent, since the parameter list and results set(s) returned from a procedure represent its contract, and the innards can be changed around, just so long as that contract is still met.  and also, if linq is used for more complex queries, tuning the database becomes a much more difficult task.  when a stored procedure is running slow, the dba can totally focus on the code in isolation, and has lots of options, just so that contract is still satisfied when he/she is done.  i have seen many, many cases where serious problems in an application were addressed by changes to the schema and code in stored procedures without any change to deployed, compiled code.  perhaps a hybird approach would be nice with linq   linq can, of course, be used to call stored procedures. ",linq linq-to-sql stored-procedures
97,"early software was often modal, but usability took a turn at some point, away from this style.   vi-based editors are total enigmas -- they're the only real surviving members of that order of software.   modes are a no-no in usability and interaction design because we humans are fickle mammals who cannot be trusted to remember what mode the application is in.   if you think you are in one ""mode"" when you are actually in another, then all sorts of badness can ensue. what you believe to be a series of harmless keystrokes can (in the wrong mode) cause unlimited catastrophe. this is known as a ""mode error"".  to learn more, search for the term ""modeless"" (and ""usability"")  as mentioned in the comments below, a modal interface in the hands of an experienced and non-fickle person can be extremely efficient. ",vim editor vi
98,"creating and using pop-up menus with glut is very simple. here is a code sample that creates a pop-up menu with 4 options:  // menu items enum menu_type {         menu_front,         menu_spot,         menu_back,         menu_back_front, };  // assign a default value menu_type show = menu_back_front;  // menu handling function declaration void menu(int);  int main() {         // ...          // create a menu         glutcreatemenu(menu);          // add menu items         glutaddmenuentry(""show front"", menu_front);         glutaddmenuentry(""show back"", menu_back);         glutaddmenuentry(""spotlight"", menu_spot);         glutaddmenuentry(""blend 'em all"", menu_back_front);          // associate a mouse button with menu         glutattachmenu(glut_right_button);          // ...          return; }  // menu handling function definition void menu(int item) {         switch (item)         {         case menu_front:         case menu_spot:         case menu_depth:         case menu_back:         case menu_back_front:                 {                         show = (menu_type) item;                 }                 break;         default:                 {       /* nothing */       }                 break;         }          glutpostredisplay();          return; }  ",opengl glut menu
99,"this is all described in the adobe flex 3 programming actionscript 3 pdf on page 550 (chapter 27: flash player security / cross-scripting):     if two swf files written with actionscript 3.0 are served from different domainsâfor example, http://sitea.com/swfa.swf and http://siteb.com/swfb.swfâthen, by default, flash player does not allow swfa.swf to script swfb.swf, nor swfb.swf to script swfa.swf. a swf file gives permission to swf files from other domains by calling security.allowdomain(). by calling security.allowdomain(""sitea.com""), swfb.swf gives swf files from sitea.com permission to script it.   it goes on in some more detail, with diagrams and all. ",flex flash actionscript-3 security
100,"i found one possible solution in the color faq. the luminance component y (from the cie xyz system) captures what is most perceived by humans as color in one channel. so, use those coefficients:  mono = (0.2125 * color.r) + (0.7154 * color.g) + (0.0721 * color.b);  ",colors rgb monochrome
101,"there's a simple solution.  assuming your (non-display) textures are 1024x1024 and you are restricted to a 256x256 window/display.  unsigned int win_width = 256; unsigned int win_height = win_width; unsigned int tex_width = 1024; unsigned int tex_height = tex_width;   use the window size to create your opengl window:  glutinitwindowsize(win_width, win_height);   but, use the texture size for everything else:  glviewport(0, 0, tex_width, tex_height); gluortho2d(0.0, tex_width, 0.0, tex_height); gltexcoord2i(tex_width, tex_height);  ",opengl textures
102,"drop into a cmd instance (or indeed powershell itself) and type this:  powershell -    you'll see that powershell.exe has a ""-noexit"" parameter which tells it not to exit after executing a ""startup command"". ",powershell batch-file
103,"it's the difference between ""black box"" testing (where you know what the code is supposed to do, but not how it works), and ""white box"" testing (where knowing how it works drives how you test it). ""black box"" testing is what most people think of when you mention quality assurance.  i work for a company where the qa team are also software developers.  (that narrows the field a lot if you care to guess the company.)  i know joel's opinion, and my experience leads me to partially disagree: for the same reason that a ""white hat"" hacker is more effective finding security holes, certain kinds of errors are more effectively found by white box testers who know how to write code (and therefore what the common mistakes are - for example, resource management issues like memory leaks).    also, since qa-oriented developers are part of the process from the initial design phase, they can theoretically help to drive higher-quality code throughout the process.  ideally, for each developer working on the project with a mental focus on functionality, you have an opposing developer with a mental focus on breaking the code (and thus making it better).    seen in that light, it's less a matter of using developers for testers than it is kind of disconnected pair-programming where one developer has an emphasis on controlling quality.    on the other hand, a lot of testing (such as basic ui functionality) frankly doesn't need that kind of skill.  that's where joel has a point.   for many businesses, i could see a system where programming teams trade off code review and testing duties for each others' code.  members of the business logic team, for example, could spend an occasional tour testing and reviewing code for the ui team, and vice-versa.  that way you're not ""wasting"" developer talent on full-time testing, but you are gaining the advantages of exposing the code to (hopefully) expert scrutiny and punishment.  then, a more traditional qa team can take up the ""black box"" testing. ",unit-testing testing process
104,"well, as with all ""what might be faster in real life"" questions, you can't beat a real life test.  function timefunc($function, $runs) {   $times = array();    for ($i = 0; $i &lt; $runs; $i++)   {     $time = microtime();     call_user_func($function);     $times[$i] = microtime() - $time;   }    return array_sum($times) / $runs; }  function method1() {    $foo = 'some words';   for ($i = 0; $i &lt; 10000; $i++)     $t = ""these are $foo""; }  function method2() {   $foo = 'some words';   for ($i = 0; $i &lt; 10000; $i++)     $t = ""these are {$foo}""; }  function method3()  {   $foo = 'some words';   for ($i = 0; $i &lt; 10000; $i++)     $t = ""these are "" . $foo; }  print timefunc('method1', 10) . ""\n""; print timefunc('method2', 10) . ""\n""; print timefunc('method3', 10) . ""\n"";   give it a few runs to page everything in, then...  0.0035568  0.0035388  0.0025394  so, as expected, the interpolation are virtually identical (noise level differences, probably due to the extra characters the interpolation engine needs to handle). straight up concatenation is about 66% of the speed, which is no great shock. the interpolation parser will look, find nothing to do, then finish with a simple internal string concat. even if the concat were expensive, the interpolator will still have to do it, after all the work to parse out the variable and trim/copy up the original string.  updates by somnath:  i added method4() to above real time logic.  function method4()  {   $foo = 'some words';   for ($i = 0; $i &lt; 10000; $i++)     $t = 'these are ' . $foo; }  print timefunc('method4', 10) . ""\n"";  results were:  0.0014739 0.0015574 0.0011955 0.001169   when you are just declaring a string only and no need to parse that string too, then why to confuse php debugger to parse. i hope you got my point. ",php optimization performance
105,"we use a case tool at my current company for code generation and we are trying to move away from it.  the benefits that it brings - a graphical representation of the code making components 'easier' to pick up for new developers - are outweighed by the disadvantges in my opinion.  those main disadvantages are:   we cannot do automatic merges, making it close to impossible for parallel development on one component. developers get dependant on the tool and 'forget' how to handcode.  ",cad
106,"your assumption is right, this is the optimal way to do it and it's called upsert/merge.  importance of upsert - from sqlservercentral.com:      for every update in the case mentioned above we are removing one   additional read from the table if we   use the upsert instead of exists.   unfortunately for an insert, both the   upsert and if exists methods use the   same number of reads on the table.   therefore the check for existence   should only be done when there is a   very valid reason to justify the   additional i/o. the optimized way to   do things is to make sure that you   have little reads as possible on the   db.      the best strategy is to attempt the   update. if no rows are affected by the   update then insert. in most   circumstances, the row will already   exist and only one i/o will be   required.   edit:  please check out this answer and the linked blog post to learn about the problems with this pattern and how to make it work safe. ",sql sql-server stored-procedures upsert
107,"@rob: i disagree.  to enforce what you are asking for i think you would need to use negative-look-behind, which is possible but is certainly not related to use {1}.  neither version of the regexp address that particular issue.  to let the code speak:  tibook 0 /home/jj33/swap &gt; cat text text this is &lt;http://example.com&gt; text this is text this is &lt;http://http://example.com&gt; text this is tibook 0 /home/jj33/swap &gt; cat p # /usr/bin/perl  my $re1 = '((mailto\:|(news|(ht|f)tp(s ))\://){1}\s+)'; my $re2 = '((mailto\:|(news|(ht|f)tp(s ))\://)\s+)';  while (&lt;&gt;) {   print ""evaluating: $_"";   print ""re1 saw \$1 = $1\n"" if (/$re1/);   print ""re2 saw \$1 = $1\n"" if (/$re2/); } tibook 0 /home/jj33/swap &gt; cat text | perl p evaluating: text this is &lt;http://example.com&gt; text this is re1 saw $1 = &lt;http://example.com&gt; re2 saw $1 = &lt;http://example.com&gt; evaluating: text this is &lt;http://http://example.com&gt; text this is re1 saw $1 = &lt;http://http://example.com&gt; re2 saw $1 = &lt;http://http://example.com&gt; tibook 0 /home/jj33/swap &gt;   so, if there is a difference between the two versions, it's doesn't seem to be the one you suggest. ",regex
108,"think of source control as a giant ""undo"" button for your source code. every time you check in, you're adding a point to which you can roll back. even if you don't use branching/merging, this feature alone can be very valuable.  additionally, by having one 'authoritative' version of the source control, it becomes much easier to back up.  centralized vs. distributed... the difference is really that in distributed, there isn't necessarily one 'authoritative' version of the source control, although in practice people usually still do have the master tree.  the big advantage to distributed source control is two-fold:   when you use distributed source control, you have the whole source tree on your local machine. you can commit, create branches, and work pretty much as though you were all alone, and then when you're ready to push up your changes, you can promote them from your machine to the master copy. if you're working ""offline"" a lot, this can be a huge benefit. you don't have to ask anybody's permission to become a distributor of the source control. if person a is running the project, but person b and c want to make changes, and share those changes with each other, it becomes much easier with distributed source control.  ",version-control
109,"what dale ragan said; it installed flawlessly on our windows server 2008 machine, including the dashboard running on iis 7. just give it a shot; should work fine. ",iis-7 cruisecontrol.net
110,"at my company we've recently adopted the (commercial) atlassian stack - including jira for issue tracking and bamboo for builds. much like the microsoft world (i'm guessing - we're a java shop), if you get all your products from a single vendor you get the bonus of tight integration.  for an example of how they've done interoperability, view their interoperability page.  enough shilling. generally speaking, i can summarize their general approach as:   create issues in your bug tracker (ex: issue key of proj-123). when you commit code, add ""proj-123"" to your commit comment to indicate what bug this code change fixes. when your ci server checks out the code, scan the commit comments of the diffs. record any strings matching the regex of your issue keys. when the build completes, generate a report of what issue keys were found.   specifically to your second problem:  your ci doesn't doesn't have to put anything into your bug tracker. bamboo doesn't put anything into jira. instead, the atlassian folks have provided a plugin to jira that will make a remote api call into bamboo, asking the question ""bamboo, to what builds am i (a jira issue) related "". this is probably best explained with a screenshot. ",continuous-integration bug-tracking fogbugz cruisecontrol.rb
111,"it's possible for a thread to be in more than one state at once therefore the threadstate property is actually a bitmap of possible states. so testing for equality with just one state will not give you the right result. you would need to do something like:  if((mthread.threadstate &amp; threadstate.running)  = 0)   however, checking thread state is the wrong to do anything. i'm not entirely clear what you're trying to achieve but i will guess that you're waiting for a thread to terminate before restarting it. in that case you should do:  mthread.join(); mthread = new thread(new parameterizedthreadstart(monitor)); if(check)     mthread.start(60000); else     mthread.start(0);   although if you describe the problem you're trying to solve in more detail i'm almost certain there will be a better solution. waiting around for a thread to end just to restart it again doesn't seem that efficient to me. perhaps you just need some kind of inter-thread communication   john. ",c# .net multithreading exception
112,"it depends on how often you are going to be calling the web service.  if you're going to be calling it almost constantly, it would probably be better to use method #2.  however, if it's not going to be getting called quite so often, you are better off using method #1, and only instantiating it when you need it. ",web-services
113,"by default (in c#), passing an object to a function actually passes a copy of the reference to that object. changing the parameter itself only changes the value in the parameter, and not the variable that was specified.  void test1(string param) {     param = ""new value""; }  string s1 = ""initial value""; test1(s1); // s1 == ""initial value""   using out or ref passes a reference to the variable specified in the call to the function. any changes to the value of an out or ref parameter will be passed back to the caller.  both out and ref behave identically except for one slight difference: ref parameters are required to be initialised before calling, while out parameters can be uninitialised. by extension, ref parameters are guaranteed to be initialised at the start of the method, while out parameters are treated as uninitialised.  void test2(ref string param) {     param = ""new value""; }  void test3(out string param) {     // use of param here will not compile     param = ""another value""; }  string s2 = ""initial value""; string s3; test2(ref s2); // s2 == ""new value"" // test2(ref s3); // passing ref s3 will not compile test3(out s2); // s2 == ""another value"" test3(out s3); // s3 == ""another value""   edit: as dp points out, the difference between out and ref is only enforced by the c# compiler, not by the clr. as far as i know, vb has no equivalent for out and implements ref (as byref) only, matching the support of the clr. ",c# .net vb.net
114,"most php sites should have a file (i call it a header) that you include on every single page of the site. if you put that first line of code in the header file, then include it like this on every page:   include 'header.php';   you won't have to use the global keyword or anything, the second line of code you wrote should work.  edit: oh sorry, that won't work inside functions... now i see your problem.  edit #2: ok, take my original advice with the header, but use a define() rather than a variable. those work inside functions after being included. ",php
115,"i know more about mssql that mysql, but i don't think the number of joins or number of rows you are talking about should cause you too many problems with the correct indexes in place.  have you analyzed the query plan to see if you are missing any   http://dev.mysql.com/doc/refman/5.0/en/explain.html  that being said, once you are satisifed with your indexes and have exhausted all other avenues, de-normalization might be the right answer.  if you just have one or two queries that are problems, a manual approach is probably appropriate, whereas some sort of data warehousing tool might be better for creating a platform to develop data cubes.  here's a site i found that touches on the subject:  http://www.meansandends.com/mysql-data-warehouse/ link_body%2fbody=%7bincl%3aaggregation%7d  here's a simple technique that you can use to keep denormalizing queries simple, if you're just doing a few at a time (and i'm not replacing your oltp tables, just creating a new one for reporting purposes).  let's say you have this query in your application:  select a.name, b.address from tbla a  join tblb b on b.fk_a_id = a.id where a.id=1   you could create a denormalized table and populate with almost the same query:  create table tbl_ab (a_id, a_name, b_address);  -- (types elided)   notice the underscores match the table aliases you use  insert tbl_ab select a.id, a.name, b.address from tbla a join tblb b on b.fk_a_id = a.id  -- no where clause because you want everything   then to fix your app to use the new denormalized table, switch the dots for underscores.    select a_name as name, b_address as address  from tbl_ab where a_id = 1;   for huge queries this can save a lot of time and makes it clear where the data came from, and you can re-use the queries you already have.  remember, i'm only advocating this as the last resort.  i bet there's a few indexes that would help you.  and when you de-normalize, don't forget to account for the extra space on your disks, and figure out when you will run the query to populate the new tables.  this should probably be at night, or whenever activity is low.  and the data in that table, of course, will never exactly be up to date.  [yet another edit]  don't forget that the new tables you create need to be indexed too   the good part is that you can index to your heart's content and not worry about update lock contention, since aside from your bulk insert the table will only see selects. ",mysql database denormalization
116,"check out the netflix contest.  i believe they exposed their database, or a large subset, to facilitate the contest.  update: their faq says they have 100 million entries in the subset you can download. ",resources dataset
117,"don't try to be as dynamic as php is. you could try to first define what you need.  interface season {     public string getdays(); }  interface user {     public season getwinter();     public season getspring();     public season getsummer();     public season getfall(); }  interface usermap {     public user getuser(string name); }   and please, read the documentation of hashtable before using it. this class is synchronized which means that each call is protected against multithreading which really slows the access when you don't need the extra protection. please use any map implementation instead like hashmap or treemap. ",java php jsp tomcat
118,"you'll want to setup launch condition in your deployment project to make sure version 2.0 sp1 is installed. you'll want to set a requirement based off the msinetassemblysupport variable, tied to the version number of .net 2.0 sp1 (2.0.50727.1433 and above according to this page.)  bootstrapping the project to actually download the framework if it isn't installed is a different matter, and there are plenty of articles out there on how to do that. ",.net-2.0 installer dependencies
119,"i believe this is what you want.  wmi code creator  a part of this nifty utility allows you to browse namespaces/classes/properties on the local and remote pcs, not to mention generating wmi code in vbscript/c#/vb on the fly.  very useful.  also, the source code used to create the utility is included in the download, which could provide a reference if you wanted to create your own browser like interface. ",vbscript wmi
120,maybe try this question at http://www.codeplex.com/n2/thread/list.aspx  they might be able to tell you about performance limitations or bottlenecks. ,asp.net performance .net-3.5 content-management-system n2
121,"you could use martin fowlers servicelocator pattern. in php it could look like this:  class servicelocator {   private static $soleinstance;   private $globalsettings;    public static function load($locator) {     self::$soleinstance = $locator;   }    public static function globalsettings() {     if ( isset(self::$soleinstance-&gt;globalsettings)) {       self::$soleinstance-&gt;setglobalsettings(new globalsettings());     }     return self::$soleinstance-&gt;globalsettings;   } }   your production code then initializes the service locator like this:  servicelocator::load(new servicelocator());   in your test-code, you insert your mock-settings like this:  servicelocator s = new servicelocator(); s-&gt;setglobalsettings(new mockglobalsettings()); servicelocator::load(s);   it's a repository for singletons that can be exchanged for testing purposes. ",language-agnostic oop
122,i use elmah.  it has some really nice features and here is a codeproject article on it.  i think the stackoverflow team uses elmah also  ,asp.net error-handling nlog health-monitoring
123,"java doesn't make this as pleasant as other languages, unfortunately. here's what i did:  import java.io.*; import java.util.*;  public class exectest {     public static void main(string[] args) throws ioexception {         process result = runtime.getruntime().exec(""traceroute -m 1 www.amazon.com"");          bufferedreader output = new bufferedreader(new inputstreamreader(result.getinputstream()));         string thisline = output.readline();         stringtokenizer st = new stringtokenizer(thisline);         st.nexttoken();         string gateway = st.nexttoken();         system.out.printf(""the gateway is %s\n"", gateway);     } }   this presumes that the gateway is the second token and not the third. if it is, you need to add an extra st.nexttoken(); to advance the tokenizer one more spot. ",java sockets ip router
124,"typically a byte array is sent as a base64 encoded string, not as individual bytes in tags.   http://en.wikipedia.org/wiki/base64  the base64 encoded version is about 137% of the size of the original content. ",xml web-services
125,lightspeed allows you to specify an inamingstrategy that lets you resolve table names dynamically at runtime. ,.net orm
126,you can actually script a fair number of tasks in ms virtual server:  http://www.microsoft.com/technet/scriptcenter/scripts/vs/default.mspx mfr=true  http://msdn.microsoft.com/en-us/library/aa368876(vs.85).aspx  also virtual pc guy has got a ton of stuff on his blog about scripting virtual server/pc and now hyper-v here:  http://blogs.msdn.com/virtual_pc_guy/default.aspx  vmware has similar capabilities:  http://www.vmware.com/support/developer/scripting-api/ ,msbuild virtualization hyper-v
127,this is because the font used in the tooltip doesn't include the characters you are trying to display. try installing a font pack that includes those characters. i'm affraid you can't do much for your site's visitors other than implementating a tooltip yourself using javascript. ,internet-explorer unicode
128," just write your file parser, using whatever techniques come to mind write lots of unit tests for it to make sure all your edge cases are covered   once you've done this, you will actually have a reasonable idea of the problem/solution.  right now you just have theories floating around in your head, most of which will turn out to be misguided.  step 3: refactor mercilessly. your aim should be to delete about half of your code  you'll find that your code at the end will either resemble an existing design pattern, or you'll have created a new one. you'll then be qualified to answer this question :-) ",design-patterns fileparse
129,"see kill all active connections to a database.  the reason that the approach that adam suggested won't work is that during the time that you are looping over the active connections new one can be established, and you'll miss those. the article i linked to uses the following approach which does not have this drawback:  -- set your current connection to use master otherwise you might get an error  use master alter database yourdatabase set single_user with rollback immediate   --do you stuff here   alter database yourdatabase set multi_user  ",sql-server sql-server-2005
130,"aside from trying out visual assistx, the only other one i've tried is resharper (which i highly recommend). if you do decide to go for resharper, you'll likely notice that it's missing a spell checker for code though - however the agent smith plugin fixes that. ",.net visual-studio
131," script combining in .net 3.5 sp1 best practices for fast websites http compression (gzip) compress js / css (different than http compression, minify javascript)  yui compressor .net yui compressor    my best advice is to check out the yui content. they have some great articles that talk about things like css sprites and have some nice javascript libraries to help reduce the number of requests the browser is making. ",asp.net ajax optimization performance
132,"why won't any old proxy software work for this   why does it need to be an asp.net application   there are tons of tools out there (both windows and *nix) that will get the job done quite easily.  check squid or netproxy for starters.  if you need to integrate with iis, iisproxy looks like it would do the trick too. ",.net asp.net iis
133,"actually it ships with .net 3.5 sp1.  so yes, the stored procs can use 3.5 features and libraries. ",sql-server sql-server-2008
134,"ah, if only c supported closures...  antonio is right; if you need to pass extra parameters, you'll need to redefine your function pointer to accept the additional arguments. if you don't know exactly what parameters you'll need, then you have at least three choices:   have the last argument in your prototype be a void*. this gives you flexibility of passing in anything else that you need, but it definitely isn't type-safe. use variadic parameters (...). given my lack of experience with variadic parameters in c, i'm not sure if you can use this with a function pointer, but this gives even more flexibility than the first solution, albeit still with the lack of type safety. upgrade to c++ and use function objects.  ",c architecture pointers
135,"last epoch is when 1970 gmt   select datediff(s,'19700101 05:00:00:000',lastmodified)   see also epoch date ",tsql sqlite date
136,"i started writing up a summary of my experience with my own code generator, then went back and re-read your question and found you had already touched upon the same issues yourself, focus on the execution results instead of the code layout/look.  problem is, this is hard to test, the generated code might not be suited to actually run in the environment of the unit test system, and how do you encode the expected results   i've found that you need to break down the code generator into smaller pieces and unit test those. unit testing a full code generator is more like integration testing than unit testing if you ask me. ",c++ python unit-testing code-generation swig
137,you can create a new group with both the apache user and ftp user as members and then make the permission on the upload folder 775.  this should give both the apache and ftp users the ability to write to the files in the folder but keep everyone else from modifying them. ,php apache upload
138,"there are multiple issues here as ikvm is currently being transitioned away from the gnu classpath system to sun's openjdk.  both are licensed as gpl+exceptions to state explicitly that applications which merely use the openjdk libraries will not be considered derived works.  generally speaking, applications which rely upon components with defined specs such as this do not fall under the gpl anyway.  for example, linking against public posix apis does not trigger gpl reliance in a linux application, despite the kernel being gpl.  a similar principal will usually (the details can be tricky) apply to replacing sun's java with a foss/gpl implementation. ",java .net ikvm gnu-classpath
139,"no specific article, really, but i've found emacswiki to be full of useful information. consider checking out these entries:   cplusplus as a starting point for many c++-related articles, and cpptemplate to define a template that can give you a good skeleton when you start new files  ",c++ emacs
140,"are you using only the dragenter method  if you are trying to reject the drag while still dragging over the same component you need to use both the dragenter and dragover methods.  check out this example:  &lt; xml version=""1.0"" encoding=""utf-8"" &gt; &lt;mx:application xmlns:mx=""http://www.adobe.com/2006/mxml"" layout=""absolute""&gt;     &lt;mx:script&gt;      &lt; [cdata[       import mx.core.dragsource;       import mx.managers.dragmanager;       import mx.events.dragevent;        private function ondragenter(e:dragevent):void {        if ( e.target == lbl ) {          if ( e.localx &lt; lbl.width/2 ) {          trace(""accept"");          dragmanager.acceptdragdrop(this);         }         else {          dragmanager.acceptdragdrop(null);         }        }       }        private function dostartdrag(e:mouseevent):void {        if ( e.buttondown ) {         var ds:dragsource = new dragsource();         ds.adddata(""test"", ""text"");          dragmanager.dodrag(btn, ds, e);        }       }      ]]&gt;     &lt;/mx:script&gt;     &lt;mx:label id=""lbl"" text=""hello world "" left=""10"" top=""10"" dragenter=""ondragenter(event)"" dragover=""ondragenter(event)"" /&gt;     &lt;mx:button id=""btn"" x=""47"" y=""255"" label=""button"" mousemove=""dostartdrag(event)""/&gt; &lt;/mx:application&gt;  ",actionscript-3 flex drag-and-drop
141,"it really depends on your view of the world - i used to be in the uncoupled camp. the dal was only there to supply data to the bal - end of story.  with emerging technologies such as linq to sql and entity framework becoming a bit more popular, then the line between dal and bal have been blurred a bit. in l2s especially your dal is quite tightly coupled to the business objects as the object model has a 1-1 mapping to your database field.  like anything in software development there is no right or wrong answer. you need to understand your requirements and future requirments and work from there. i would no more use a ferrari on the dakhar rally as i would a range rover on a track day. ",.net n-tier
142,"for azman with asp.net, turn on impersonation in web.config (&lt;identity impersonate=""true"" username=""xx"" pasword=""xx"" /&gt;), and make sure with an ad administrator that the impersonation account has ""reader"" permissions on the azman store; plus, give write permissions to this account on the temporary asp.net files folder (under c:\windows\microsoft.net\&lt;framework&gt;). ",c# winforms authorization adam azman
143,"you should use atexit() if possible.  on_exit() is nonstandard and less common.  for example, it's not available on os x.  kernel.org - on_exit():     this function comes from sunos 4, but is also present in libc4, libc5 and   glibc.  it no longer occurs in solaris (sunos 5).  avoid this function, and   use the standard atexit(3) instead.  ",c linux
144,"the idea behind using the threadpool is that through it you can control the amount of synchronous threads, and if those get too many, then the thread pool automatically manages the waiting of newer threads.  the asp.net worked thread (afaik) doesn't come from the thread pool and shouldn't get affected by your call to the remoting service (unless this is a very slow processor, and your remoting function is very cpu intensive - in which case, everything on your computer will be affected).  you could always host the remoting service on a different physical server. in that case, your asp.net worker thread will be totally independent of your remoting call (if the remoting call is called on a separate thread that is). ",c# .net-3.5 .net-2.0 remoting rpc
145,"boost, for c++ ",regex
146,"the biggest performance boost you can get is by partitioning tables/indexes onto different disks. the first step would be to put indexes on one disk and data on an other. after this you should consider which tables/indexes are used together, and put them on separate disks (""spindles"") when possible. ",database hardware storage raid disk
147,the short answer is that early (or static) binding refers to compile time binding and late (or dynamic) binding refers to runtime binding (for example when you use reflection). ,binding dynamic-binding static-binding
148,"here's an overview of the flow in mvc:   /new - render your ""new"" view containing a form for the user to fill out user fills out form and it is posted to /create the post is routed to the create action on your controller in your action method, update the model with the data that was posted. your model should validate itself. your controller should read if the model is valid. if the model is valid, save it to your db.  redirect to /show to render the show view for your object. if the model is invalid, save the form values and error messages in the tempdata, and redirect to the new action again.  fill your form fields with the data from tempdata and show the error message(s).   the validation frameworks will help you along in this process.  also, i think the asp.net mvc team is planning a validation framework for the next preview. ",asp.net-mvc validation
149,"for asp.net, you have a lot of improvements:   split view (code and design) faster switching between code and design view embedded master pages (one master page in another) javascript debugging   anyway most of the useful stuff are really in the meat of the language, and for .net 3.5 the new language features for c# 3.0 will be (and yes, i find all of them useful)   anonymous objects automatic properties object initializers collection initializers (inline initialization for collections) implicit typing (var keyword) lambda expressions linq extension methods   i might have forgotten a few, but i think this is about most of the new cool and useful stuff. ",asp.net
150,"yes, it depends on language, since string storage differs between languages.   pascal-type strings: length = 0. c-style strings: [0] == 0.  .net: .isnullorempty.   etc.  ",string optimization language-agnostic compiler-construction
151,"on ms sql server 2005 and above, row_number() seems to work:  t-sql: paging with row_number()  declare @pagenum as int; declare @pagesize as int; set @pagenum = 2; set @pagesize = 10;  with ordersrn as (     select row_number() over(order by orderdate, orderid) as rownum           ,orderid           ,orderdate           ,customerid           ,employeeid       from dbo.orders )  select *    from ordersrn  where rownum between (@pagenum - 1) * @pagesize + 1                    and @pagenum * @pagesize  order by orderdate          ,orderid;  ",.net sql linq pagination
152,"   what specifically does that question mean    good question.  if reversing the ""on"" bits means reversing only the bits that are ""on"", then you will always get 0, no matter what the input is.  if it means reversing all the bits, i.e. changing all 1s to 0s and all 0s to 1s, which is how i initially read it, then that's just a bitwise not, or complement.  c-based languages have a complement operator, ~, that does this.  for example:  unsigned char b = 102;      /* 0x66, 01100110 */ unsigned char reverse = ~b; /* 0x99, 10011001 */  ",language-agnostic
153,"you could also try drscheme, which whilst not exactly a standalone interpreter, isn't emacs :)  it's basically a simple ide that has an area to type in code that can be executed as a file, and then another area that is the running interpreter that you can interact with.  (also, find the uc berkeley cs61a podcasts and listen to them, as well as reading sicp) ",lisp scheme
154,as a bare minimum:  mscoree.lib msvcrt.lib mfc90.lib (adjust version appropriately)  and iterate from there. ,linker c++-cli mixed-mode
155,"declaration  a prototype for a function which takes a function parameter looks like the following:  void func ( void (*f)(int) );   this states that the parameter f will be a pointer to a function which has a void return type and which takes a single int parameter. the following function (print) is an example of a function which could be passed to func as a parameter because it is the proper type:  void print ( int x ) {   printf(""%d\n"", x); }   function call  when calling a function with a function parameter, the value passed must be a pointer to a function. use the function's name (without parentheses) for this:  func(print);   would call func, passing the print function to it.  function body  as with any parameter, func can now use the parameter's name in the function body to access the value of the parameter. let's say that func will apply the function it is passed to the numbers 0-4. consider, first, what the loop would look like to call print directly:  for ( int ctr = 0 ; ctr &lt; 5 ; ctr++ ) {   print(ctr); }   since func's parameter declaration says that f is the name for a pointer to the desired function, we recall first that if f is a pointer then *f is the thing that f points to (i.e. the function print in this case). as a result, just replace every occurrence of print in the loop above with *f:  void func ( void (*f)(int) ) {   for ( int ctr = 0 ; ctr &lt; 5 ; ctr++ ) {     (*f)(ctr);   } }   from http://math.hws.edu/bridgeman/courses/331/f05/handouts/c-c++-notes.html ",c function pointers syntax parameters
156,"if i understand the question correctly, you've created a domain model and you would like to write an object-relational mapper to map between records in your database and your domain objects. however, you're concerned about polluting your domain model with the 'plumbing' code that would be necessary to read and write to your object's fields.  taking a step back, you essentially have two choices of where to put your data mapping code - within the domain class itself or in an external mapping class. the first option is often called the active record pattern and has the advantage that each object knows how to persist itself and has sufficient access to its internal structure to allow it to perform the mapping without needing to expose non-business related fields.  e.g  public class user {  private string name;  private accountstatus status;   private user()  {  }   public string name  {   get { return name; }   set { name = value; }  }   public accountstatus status  {   get { return status; }  }   public void activate()  {   status = accountstatus.active;  }   public void suspend()  {   status = accountstatus.suspended;  }   public static user getbyid(int id)  {   user fetcheduser = new user();    // lots of database and error-checking code   // omitted for clarity   // ...    fetcheduser.name = (string) reader[""name""];   fetcheduser.status = (int)reader[""statuscode""] == 0   accountstatus.suspended : accountstatus.active;    return fetcheduser;  }   public static void save(user user)  {   // code to save user's internal structure to database   // ...  } }   in this example, we have an object that represents a user with a name and an accountstatus. we don't want to allow the status to be set directly, perhaps because we want to check that the change is a valid status transition, so we don't have a setter. fortunately, the mapping code in the getbyid and save static methods have full access to the object's name and status fields.  the second option is to have a second class that is responsible for the mapping. this has the advantage of seperating out the different concerns of business logic and persistence which can allow your design to be more testable and flexible. the challenge with this method is how to expose the name and status fields to the external class. some options are:   1. use reflection (which has no qualms about digging deep into your object's private parts)   2. provide specially-named, public setters (e.g. prefix them with the word 'private') and hope no one uses them accidentally   3. if your language suports it, make the setters internal but grant your data mapper module access. e.g. use the internalsvisibletoattribute in .net 2.0 onwards or friend functions in c++  for more information, i'd recommend martin fowler's classic book 'patterns of enterprise architecture'  however, as a word of warning, before going down the path of writing your own mappers i'd strongly recommend looking at using a 3rd-party object relational mapper (orm) tool such as nhibernate or microsoft's entity framework. i've worked on four different projects where, for various reasons, we wrote our own mapper and it is very easy to waste a lot of time maintaining and extending the mapper instead of writing code that provides end user value. i've used nhibernate on one project so far and, although it has quite a steep learning curve initially, the investment you put in early on pays off considerably. ",architecture
157,"you should check out the qtkit capture documentation.  on leopard, you can get at all of it over the rubycocoa bridge:  require 'osx/cocoa' osx.require_framework(""/system/library/frameworks/qtkit.framework"")  osx::qtcapturedevice.inputdevices.each do |device|     puts device.localizeddisplayname end  ",objective-c osx webcam isight
158,"unfortunately, the ""full"" sharepoint experience is limited to running internet explorer 6/7 and office 2007.  on the mac, i recommend using firefox (camino ) which seems to work a bit better than safari.  edit: when you say ""similar experience"", what exactly are you missing  i don't have any mac here, but i was under the impression that office 2008 will have a working integration with sharepoint as well. ",sharepoint osx wss
159,"i believe you can set the desired solution up by rerunning the vmware configuration script.  and doing a custom network setup, so that both nic's are mapped to your vmware instance.  i would recommend making eth0 the 2nd nic since it will be easier for linux to use by default.  then make eth1 the 1st nic. ",linux vmware sysadmin nic
160,"answered my own question:  use the netserverenum function, passing in the sv_type_domain_enum constant for the ""servertype"" argument.  in delphi, the code looks like this:  &lt;snip&gt; type   net_api_status = dword;   pserver_info_100 = ^server_info_100;   server_info_100 = packed record     sv100_platform_id : dword;     sv100_name        : pwidechar; end;  function netserverenum(  //get a list of pcs on the network (same as dos cmd ""net view"")   const servername    : pwidechar;   const level         : dword;   const bufptr        : pointer;   const prefmaxlen    : dword;   const entriesread   : pdword;   const totalentries  : pdword;   const servertype    : dword;   const domain        : pwidechar;   const resume_handle : pdword ) : net_api_status; stdcall; external 'netapi32.dll';  function netapibufferfree(  //memory mgmt routine   const buffer : pointer ) : net_api_status; stdcall; external 'netapi32.dll';  const   max_preferred_length = dword(-1);   nerr_success = 0;   sv_type_all  = $ffffffff;   sv_type_domain_enum = $80000000;   function tnetwork.computersindomain: tstringlist; var   pbuffer        : pserver_info_100;   pwork          : pserver_info_100;   dwentriesread  : dword;   dwtotalentries : dword;   i              : integer;   dwresult       : net_api_status; begin   result := tstringlist.create;   result.clear;    dwresult := netserverenum(nil,100,@pbuffer,max_preferred_length,                             @dwentriesread,@dwtotalentries,sv_type_domain_enum,                             pwidechar(fdomainname),nil);    if dwresult = nerr_success then begin     try       pwork := pbuffer;       for i := 1 to dwentriesread do begin         result.add(pwork.sv100_name);         inc(pwork);       end;  //for i     finally       netapibufferfree(pbuffer);     end;  //try-finally   end  //if no error   else begin     raise exception.create('error while retrieving computer list from domain ' +                            fdomainname + #13#10 +                            syserrormessage(dwresult));   end; end; &lt;snip&gt;  ",winapi
161,"copy any applicable preferences files in ~/library/preferences from a machine that you have checked ""don't show again"" on. ",osx adobe applescript acrobat
162,"there is an actual data type called keyvaluepair, use like this  keyvaluepair&lt;string, string&gt; mykeyvaluepair = new keyvaluepair&lt;string,string&gt;(""defaultkey"", ""defaultvalue"");  ",c# data-structures collections
163,"i've got something similar set up: i have a main ant build.xml which calls a separate build.xml that takes care of building my tests. this is how i do it:  &lt;target name=""build-tests""&gt;     &lt;subant target=""build""&gt;       &lt;fileset dir=""${test.home}"" includes=""build.xml""/&gt;     &lt;/subant&gt; &lt;/target&gt;   the trick is to use subant instead of antcall. you don't have to import the other build file. ",ant build-process
164,"i've used event-based apis for plugins in the past. you can insert hooks for plugins by dispatching events and providing access to the application state.  for example, if you were writing a blogging application, you might want to raise an event just before a new post is saved to the database, and provide the post html to the plugin to alter as needed. ",plugins extensibility
165,"linq stands for language integrated query and is a set of extensions for .net that allow you to query data the same way from code and isn't tied to a specific data source.  you can use the same linq code for sql server, xml, objects, datasets, and entities.  here is a good intro from scott guthrie  this is a nice set of 101 linq samples ",linq .net-3.5
166,"trust me, you will spend more money trying to search/develop a solution for this as compared to buying a third party component. do not reinvent the wheel and go for the paid solution.  printing is a complex problem and i would love to see the day when better framework support is added for this. ",c# .net windows-services printing
167,"a) must look the same  b) as standards-compliant as possible, but not so anal that it blocks finishing work  in a situation where you have perpetual access to the code, i don't think standards-compliance is all that important, since you can always make changes to the code if something breaks. if you don't have perpetual access (ie, you sign off on the code and it becomes someone else's responsibility), it's probably best to be as standards-compliant as possible to minimize maintenance headaches later... even if you never have to deal with the code again, your reputation persists and can be transmitted to other potential clients, and many teams like to blame the previous developer(s) for problems that come up. ",css validation xhtml cross-browser w3c
168,"you need to use an ifstream if you just want to read (use an ofstream to write, or an fstream for both).  to open a file in text mode, do the following:  ifstream in(""filename.ext"", ios_base::in); // the in flag is optional   to open a file in binary mode, you just need to add the ""binary"" flag.  ifstream in2(""filename2.ext"", ios_base::in | ios_base::binary );    use the ifstream.read() function to read a block of characters (in binary or text mode).  use the getline() function (it's global) to read an entire line. ",c++ file io
169,"maybe you should take a look at izpack. i created a very nice installer some years ago and i'd bet that they are still improving it. it allows the installation of docs, binaries and a clickable link to start the application iirc. ",java windows swt executable software-distribution
170,"it's probably the awesome firefox3 fsync ""bug"", which is a giant pile of fail.  in summary   firefox3 saves its bookmarks and history in an sqlite database every time you load a page it writes to this database several times sqlite cares deeply that you don't lose your bookmarks, so each time it writes, instructs the kernel to flush it's database file to disk and ensure that it's fully written many variants of linux, when told to flush like that, flush every file. this may take up to a minute or more if you have background tasks doing any kind of disk intensive stuff. the kernel makes firefox wait while this flush happens, which locks up the ui.  ",javascript firefox memory cpu
171,"my eventual (and hopefully temporary) solution to this problem was a python cgi script.  my script follows for anyone who might find it useful (despite the fact that this is a total hack).    # /usr/bin/python """"""a cgi script to produce an rss feed of top-level gallery2 albums.""""""  #import cgi #import cgitb; cgitb.enable() from time import gmtime, strftime import mysqldb  album_query = '''     select g_id, g_title, g_originationtimestamp     from g_item     where g_cancontainchildren = 1      order by g_originationtimestamp desc     limit 0, 20     '''  rss_template = '''content-type: text/xml  &lt; xml version=""1.0"" &gt; &lt;rss version=""2.0""&gt;   &lt;channel&gt;     &lt;title&gt;title&lt;/title&gt;     &lt;link&gt;&lt;http://example.com/gallery2/main.php&gt;&lt;/link&gt;     &lt;description&gt;description&lt;/description&gt;     &lt;ttl&gt;1440&lt;/ttl&gt; %s   &lt;/channel&gt; &lt;/rss&gt; '''  item_template = '''     &lt;item&gt;       &lt;title&gt;%s&lt;/title&gt;       &lt;link&gt;&lt;http://example.com/gallery2/main.php g2_itemid=%s&gt;&lt;/link&gt;       &lt;description&gt;%s&lt;/description&gt;       &lt;pubdate&gt;%s&lt;/pubdate&gt;     &lt;/item&gt; '''  def to_item(row):     item_id = row[0]     title = row[1]     date = strftime(""%a, %d %b %y %h:%m:%s gmt"", gmtime(row[2]))     return item_template % (title, item_id, title, date)  conn = mysqldb.connect(host = ""host"",                        user = ""user"",                        passwd = ""password"",                        db = ""database"") curs = conn.cursor() curs.execute(album_query) print rss_template % ''.join([ to_item(row) for row in curs.fetchall() ]) curs.close()  ",php web-applications
172,"we use watin at my place of employment, we are a .net shop so this solution made a lot of sense.  we actually started with watir (the original ruby implementation) and switched after.  it's been a pretty good solution for us so far ",unit-testing user-interface selenium
173,"if you have ssh installed, you should be able to run..  ssh-keygen   then go through the steps, you'll have two files, id_rsa and id_rsa.pub (the first is your private key, the second is your public key - the one you copy to remote machines)  then, connect to the remote machine you want to login to, to the file ~/.ssh/authorized_keys add the contents of your that id_rsa.pub file.  oh, and chmod 600 all the id_rsa* files (both locally and remote), so no other users can read them:  chmod 600 ~/.ssh/id_rsa*   similarly, ensure the remote ~/.ssh/authorized_keys file is chmod 600 also:  chmod 600 ~/.ssh/authorized_keys   then, when you do ssh remote.machine, it should ask you for the key's password, not the remote machine.    to make it nicer to use, you can use ssh-agent to hold the decrypted keys in memory - this means you don't have to type your keypair's password every single time. to launch the agent, you run (including the back-tick quotes, which eval the output of the ssh-agent command)  `ssh-agent`   on some distros, ssh-agent is started automatically. if you run echo $ssh_auth_sock and it shows a path (probably in /tmp/) it's already setup, so you can skip the previous command.  then to add your key, you do  ssh-add ~/.ssh/id_rsa   and enter your passphrase. it's stored until you remove it (using the ssh-add -d command, which removes all keys from the agent) ",linux ssh
174,"we use cruisecontrol with nunit, ncover, fxcop, svn and some custom tools we wrote ourselves to produce the reports. in my opinion it has proven (over the last few years) to be an excellent combination.  it's frustrating that ms restricts all of its integration tools to vsts.  its test framework is as good as nunit, but you can't use its code coverage tools or anything else.  i'd check out xnuit - it's looking pretty promising (but currently lacking ui).  we automate nightly builds, and you could automate uat and manual test builds, but i'm not sure that we'd ever want to automate the release to our production servers. even if it were any change would be important enough that someone would have to watch over it anyway. ",svn continuous-integration
175,silverlight springs to mind as an obvious choice if you want to do animation using .net on the web. it may not cover all platforms but will work in ie and firefox and on the mac. ,.net animation
176,"how much order do you need to impose on the threads  if you just need all of the work started in the loop to finish before the code continues, but you don't care about the order the work within the loop finishes, then calling join is the answer. to add more detail to kevin kenny's answer, you should call join outside the loop. this means you will need a collection to hold references to the threads you started:  // start all of the threads. list&lt;thread&gt; startedthreads = new list&lt;thread&gt;(); foreach (...) {   thread thread = new thread(new threadstart(mymethod));   thread.start();   startedthreads.add(thread); }  // wait for all of the threads to finish. foreach (thread thread in startedthreads) {   thread.join(); }   in contrast, if you called join inside the loop, the result would basically be the same as not using threads at all. each iteration of the loop body would create and start a thread but then immediately join it and wait for it to finish.  if the individual threads produce some result (write a message in a log, for example) then the messages may still appear out of order because there's no coordination between the threads. it is possible to get the threads to output their results in order by coordinating them with a monitor. ",c# multithreading compact-framework
177,app_data folder on the root of the project. it isn't served to web requests; so other people can't snoop for it.,asp.net logging permissions
178,"that's one of the sad reasons i'm still targeting .net 2.0 whenever possible :/  but people don't neccessarily need the full 200 mb package. there is a 3 mb bootstrapper which will only download the required components:  .net 3.5 sp1 bootstrapper  however, the worst case scenario is still a pretty hefty download. also, see this article for a more detailed explanation on the size and an alternative workaround to the size problem.  addition: since answering this question, scott hanselman created smallestdotnet.com, which will determine the smallest required download. doesn't change the worst case scenario, but is still useful to know. ",.net redistributable
179,foreach(var row in datagrid1.rows) {   dostuff(row); } //or ---------------------------------------------    foreach(datagridrow row in datagrid1.rows) {   dostuff(row); } //or --------------------------------------------- for(int i = 0; i&lt; datagrid1.rows.count - 1; i++) {   dostuff(datagrid1.rows[i]); }  ,winforms
180,"you defined the primary key twice. try:  create table shipping_grid(       id int not null auto_increment primary key comment 'unique id for each row',       shipping_vendor_no int(6) not null comment 'foreign key to vendor.no for the shipping vendor (vendors_type must be 3)',       start_vendor_no int(6) not null comment 'foreign key to vendor.no for the vendor being shipped from',       end_vendor_no int(6) not null comment 'foreign key to the vendor.no for the vendor being shipped to',       shipment_duration int(1) default 1 comment 'duration in whole days shipment will take',       price float(5,5) not null comment 'price in us dollars per shipment lbs (down to 5 decimal places)',       is_flat_rate tinyint(1) default 0 comment '1 if is flat rate regardless of weight, 0 if price is by lbs',       index (shipping_vendor_no),       index (start_vendor_no),       index (end_vendor_no),       foreign key (shipping_vendor_no) references vendor (no),       foreign key (start_vendor_no) references vendor (no),       foreign key (end_vendor_no) references vendor (no)   ) type = innodb;   the vendor primary key must be int(6), and both tables must be of type innodb. ",sql mysql foreign-keys
181,"from a high level, yes.  :d  the main thing is some place to store the templates.  a database is a great option unless you're not already using one, then file systems work fine.    wsiwig editors (such as fckeditor) work well and give you some good options regarding the features that you allow.    some sort of token replacement system is also a good idea if you need it.  for example, if someone puts %firstname% in the email template, the code that generates the email can do some simple pattern matching to replace known tokens with other known values that may be dynamic based on user or other circumstances.",.net email
182,"the file in the .svn directory that keeps track of what you have checked out, when, what revision, and from where, has gotten corrupted somehow, for that particular file.    this is no more dangerous or critical than the normal odd file problem, and can be because of various problems, like a subversion program dying mid-change, power-disruption, etc.    unless it happens more i wouldn't make much out of it.    it can be fixed by doing what you did, make a copy of your work-files, check out a fresh copy, and add the modified files back in.    note that this might cause problems if you have a busy project where you would normally have to merge in changes.    for instance, you and a collegue both check out a fresh copy, and start working on the same file. at some point, your collegue checks in his modifications. when you attempt to do the same, you get the checksum problem you have. if you now make copies of your changed files, do a fresh checkout, then subversion will lose track of how your changes should be merged back in.    if you didn't get the problem in this case, when you got around to checkin in your modifications, you would need to update your working copy first, and possibly handle a conflict with your file.    however, if you do a fresh checkout, complete with your collegues changes, it now looks like you removed his changes and substituted with your own. no conflicts, and no indications from subversion that something is amiss.",svn subclipse
183,"i've had to do this before. the only real way to do it is to manually match up the various locations. use your database's console interface and grouping select statements. first, add your ""company name"" field. then:    select count(*) as repcount, ""location name"" from mytable where ""company name"" is null group by ""location name"" order by repcount desc limit 5;    figure out what company the location at the top of the list belongs to and then update your company name field with an update ... where ""location name"" = ""the location"" statement.    p.s. - you should really break your company names and location names out into separate tables and refer to them by their primary keys.    update: - wow - no duplicates  how many records do you have ",database
184,"email address    rational      users don't change emails very often  removes the step of asking for username and email address, which you'll need anyway  users don't often forget their email address (see number one)  email will be unique unless the user already registered for the site, in which case forward them to a forgot your password screen  almost everyone is using email as the primary login for access to a website, this means the rate of adoption shouldn't be affected by the fact that you're asking for an email address          update    after registration, be sure to ask the user to create some kind of username, don't litter a public site with their email address  also, another benefit of using an email address as a login: you won't need any other information (like password / password confirm), just send them a temp password through the mail, or forgo passwords altogether and send them a one-use url to their email address every time they'd like to login (see: mugshot.org)",authentication web-applications account
185,"i guess a fairly big downside is that the extra volume of code that you have to write, manage and maintain for a small project may just be overkill.  it's all down to what's appropriate for the size of the project, the expected life of the final project and the budget   sometimes, whilst doing things 'properly' is appealing, doing something a little more 'lightweight' can be the right commercial decision  ",architecture n-tier
186,what type is selectedyear  a datetime  if so then you might need to convert to a string.,asp.net-mvc asp.net-mvc-routing
187,"when dealing with indexes, you have to determine what your table is going to be used for.  if you are primarily inserting 1000 rows a second and not doing any querying, then a clustered index is a hit to performance.  if you are doing 1000 queries a second, then not having an index will lead to very bad performance.  the best thing to do when trying to tune queries/indexes is to use the query plan analyzer and sql profiler in sql server.  this will show you where you are running into costly table scans or other performance blockers.    as for the guid vs id argument, you can find people online that swear by both.  i have always been taught to use guids unless i have a really good reason not to.  jeff has a good post that talks about the reasons for using guids: http://www.codinghorror.com/blog/archives/000817.html.    as with most anything development related, if you are looking to improve performance there is not one, single right answer.  it really depends on what you are trying to accomplish and how you are implementing the solution.  the only true answer is to test, test, and test again against performance metrics to ensure that you are meeting your goals.    [edit]  @matt, after doing some more research on the guid/id debate i came across this post.  like i mentioned before, there is not a true right or wrong answer.  it depends on your specific implementation needs.  but these are some pretty valid reasons to use guids as the primary key:        for example, there is an issue known as a ""hotspot"", where certain pages of data in a table are under relatively high currency contention. basically, what happens is most of the traffic on a table (and hence page-level locks) occurs on a small area of the table, towards the end. new records will always go to this hotspot, because identity is a sequential number generator. these inserts are troublesome because they require exlusive page lock on the page they are added to (the hotspot). this effectively serializes all inserts to a table thanks to the page locking mechanism. newid() on the other hand does not suffer from hotspots. values generated using the newid() function are only sequential for short bursts of inserts (where the function is being called very quickly, such as during a multi-row insert), which causes the inserted rows to spread randomly throughout the table's data pages instead of all at the end - thus eliminating a hotspot from inserts.        also, because the inserts are randomly distributed, the chance of page splits is greatly reduced. while a page split here and there isnt too bad, the effects do add up quickly. with identity, page fill factor is pretty useless as a tuning mechanism and might as well be set to 100% - rows will never be inserted in any page but the last one. with newid(), you can actually make use of fill factor as a performance-enabling tool. you can set fill factor to a level that approximates estimated volume growth between index rebuilds, and then schedule the rebuilds during off-peak hours using dbcc reindex. this effectively delays the performance hits of page splits until off-peak times.        if you even think you might need to enable replication for the table in question - then you might as well make the pk a uniqueidentifier and flag the guid field as rowguidcol. replication will require a uniquely valued guid field with this attribute, and it will add one if none exists. if a suitable field exists, then it will just use the one thats there.        yet another huge benefit for using guids for pks is the fact that the value is indeed guaranteed unique - not just among all values generated by this server, but all values generated by all computers - whether it be your db server, web server, app server, or client machine. pretty much every modern language has the capability of generating a valid guid now - in .net you can use system.guid.newguid. this is very handy when dealing with cached master-detail datasets in particular. you dont have to employ crazy temporary keying schemes just to relate your records together before they are committed. you just fetch a perfectly valid new guid from the operating system for each new record's permanent key value at the time the record is created.         http://forums.asp.net/t/264350.aspx  ",sql-server indexing
188,"i've done quite a few projects using community server. if you're okay with the out-of-the-box functionality, or you don't mind sticking to the version you start with, i think you'll be very happy.    the times i've run into headaches using cs is when the client wants functionality cs does not provide, but also insists on keeping the ability to upgrade to the latest version whenever telligent releases an update. you can mostly support that by making all of your changes either in a separate project or by only modifying aspx/ascx files (no codebehinds). some kind of merge is going to be required though no matter how well you plan it out.",community-server
189,"you should probably write some automated tests for each condition you can think of, and then just start brainstorming more, writing the tests as you think of them.  this way, you can see for sure it will work, and will continue to work if you make further changes.  look up test driven development if you like the results.",vb.net date
190,"i've started using linqpad. in addition to being more lightweight than ssms, you can also practice writing linq queries- way more fun than boring old tsql ",sql-server
191,"i've seen plenty of people type their email address wrong and i've also looked through user databases full of invalid email address.    the way i see it you've got two options.  use a second box to confirm the input, or send an authentication/activation email.    both are annoyances so you get to choose which you think will annoy your users less.    most would argue that having to find an email and click on a link is more annoying, but it avoids the copy/paste a bad address issue, and it allows you to do things like delete or roll back users if they don't activate after say 48 hours.",html email forms confirm
192,"the reason why #1 is slower is that [\d;]+ is a greedy quantifier. using +  or *  is going to do lazy quantifing. see msdn - quantifiers for more info.    you may want to try:    ""(\e\[(\d{1,2};)* [mz] ) ""    that may be faster for you.",regex ansi
193,can you let us know that technology you are using as there are some intresting bits out there around this stuff and some short cuts... i.e. wse2 is complex beast and something that i dont like getting wrong     i dont like developers doing this and there are wse2 accelorators out there like ssl accelorates as the processing of encryption has a hugh cost best to take it out of process from the normal code and the development arena.  if this is an option for you - try look at this - forumsystems ,xml xml-signature
194,"within your overridden processcmdkey how are you determining which key has been pressed   the value of keydata (the second parameter) will change dependant on the key pressed and any modifier keys, so, for example, pressing the left arrow will return code 37, shift-left will return 65573, ctrl-left 131109 and alt-left 262181.  you can extract the modifiers and the key pressed by anding with appropriate enum values:protected override bool processcmdkey(ref message msg, keys keydata)  {      bool shiftpressed = (keydata &amp; keys.shift)  = 0;      keys unmodifiedkey = (keydata &amp; keys.keycode);        // rest of code goes here  }  ",c# user-interface directx
195,"i use msys-git on windows every single day. works fast and flawlessly.    although the newer build has some problems with git-svn, this build (git-1.5.5-preview20080413.exe) has a working git-svn.",version-control dvcs
196,"i'm not sure if this would be handled exactly the same way for webstart, but we ran into this situation in a desktop application when dealing with a set of native libraries (dlls in our case).    loading liba before libb should work, unless one of those libraries has a dependency that is unaccounted for and not in the path. my understanding is that once it gets to a system loadlibrary call (i.e. java has found the library in its java.library.path and is now telling the os to load it) - it is completely dependent on the operating system to find any dependent libraries, because at that point it is the operating system that is loading the library for the process, and the os only knows how to look in the system path. that seems hard to set in the case of a webstart app, but there is a way around this that does not involve static compiling. you may be able to shuffle where your libraries are - i am unsure     if you use a custom classloader, you can override loadlibrary and findlibrary so that it can locate your libraries from within a jar in your classpath, and if you also make it aware of your native library dependencies (i.e. libb depends on liba depends on libx, then when loading libb you can catch yourself and ensure you load liba first, and in checking that notice and load libx first. then the os doesn't try to find a library that isn't in your path. it's klunky and a bit painful, but ensuring java finds them and loads them all in the correct order can work.",java jni java-web-start nativelibrary
197,"you can't dynamically add methods to existing objects or classes in .net, except by changing the source for that class.    you can, however, in c# 3.0, use extension methods, which look like new methods, but are compile-time magic.    to do this for your code:    public static class stringextensions{    public static string trim(this string s)    {        return s.trim();    }}    to use it:    string s = ""  test  "";s = s.trim();    this looks like a new method, but will compile the exact same way as this code:    string s = ""  test  "";s = stringextensions.trim(s);    what exactly are you trying to accomplish  perhaps there are better ways of doing what you want ",c# .net
198,i've had great success with wsdl2php.  it will automatically create wrapper classes for all objects and methods used in your web service. ,php web-services visual-studio wsdl
199,"it's acceptable as long as you understand the consequences. the main reason you'd have a bll is to re-use that logic elsewhere throughout your application.    if you have all that validation logic in the presentation code, you're really making it difficult to re-use elsewhere within your application.",asp.net business-logic-layer objectdatasource
200,"datagrid was an asp.net 1.1 control, still supported. gridview arrived in 2.0, made certain tasks simpler added different databinding features:  this link has a comparison of datagrid and gridview features -  https://msdn.microsoft.com/en-us/library/05yye6k9(v=vs.100).aspx ",asp.net
201,"there are three key components (assuming ur using sql server):   sqlconnection sqlcommand sqldatareader   (if you're using something else, replace sql with ""something"", like mysqlconnection, oraclecommand)  everything else is just built on top of that.  example 1:  using (sqlconnection connection = new sqlconnection(""connection string"")) using (sqlcommand command = new sqlcommand()) {   command.commandtext = ""select name from users where status = @onlinestatus"";   command.connection = connection;   command.parameters.add(""@onlinestatus"", sqldbtype.int).value = 1; //replace with enum   connection.open();    using (sqldatareader dr = command.executereader))   {       list&lt;string&gt; onlineusers = new list&lt;string&gt;();        while (dr.read())       {          onlineusers.add(dr.getstring(0));       }   } }   example 2:  using (sqlconnection connection = new sqlconnection(""connection string"")) using (sqlcommand command = new sqlcommand()) {   command.commandtext = ""delete from users where email = @email"";   command.connection = connection;   command.parameters.add(""@email"", sqldbtype.varchar, 100).value = ""user@host.com"";   connection.open();   command.executenonquery(); }  ",ado.net
202,"i am a big fan of embarcadero's er/studio.  it is very powerful and produces excellent on-screen as well as printed results.  they have a free trial as well, so you should be able to get in and give it a shot without too much strife.  good luck  ",sql sql-server database diagram
203,"all of the following we use and integrate easiy in both our maven 2.x builds and eclipse/rad 7:   testing - junit/testng code analysis - findbugs, pmd code coverage - clover   in addition, in our maven builds we have:   jdepend tag checker (todo, fixme, etc)   furthermore, if you're using maven 2.x, codehaus has a collection of handy maven plugins in their mojo project.  note: clover has out-of-the-box integration with the bamboo ci server (since they're both atlassian products). there are also bamboo plugins for findbugs, pmd, and checkstyle but, as noted, the free hudson ci server has those too. ",java code-coverage static-analysis
204,"you didn't mention for what os, but the wmi redistributable components version 1.0 definitely exists.    for windows server 2003, the wmi sdk and redistributables are part of the server sdk    i believe that the same is true for the server 2008 sdk",windows 64bit wmi
205,from timocracy.com:    require 'rake'  require 'rake/rdoctask'  require 'rake/testtask'  require 'tasks/rails'    def capture_stdout    s = stringio.new    oldstdout = $stdout    $stdout = s    yield    s.string  ensure    $stdout = oldstdout  end    rake.application.rake_require '../../lib/tasks/metric_fetcher'  results = capture_stdout {rake.application['metric_fetcher'].invoke}  ,ruby rake command-line-interface
206,"http://www.easymaths.com/what_on_earth_is_bodmas.htm:     what do you think the answer to 2 + 3 x 5 is       is it (2 + 3) x 5 = 5 x 5 = 25        or 2 + (3 x 5) = 2 + 15 = 17        bodmas can come to the rescue and give us rules to follow so that we always get the right answer:      (b)rackets (o)rder (d)ivision (m)ultiplication (a)ddition (s)ubtraction      according to bodmas, multiplication should always be done before addition, therefore 17 is actually the correct answer according to bodmas and will also be the answer which your calculator will give if you type in 2 + 3 x 5 .   why it is useful in programming  no idea, but i assume it's because you can get rid of some brackets  i am a quite defensive programmer, so my lines can look like this:  result = (((i + 4) - (a + b)) * magic_number) - another_magic_number;   with bodmas you can make this a bit clearer:  result = (i + 4 - (a + b)) * magic_number - another_magic_number;   i think i'd still use the first variant - more brackets, but that way i do not have to learn yet another rule and i run into less risk of forgetting it and causing those weird hard to debug errors   just guessing at that part though.  mike stone edit: fixed math as gaius points out ",math
207,"you should take a look at the unpivot clause.  update1: gatekiller, strangely enough i read an article (about something unrelated) about it this morning and i'm trying to jog my memory where i saw it again, had some decent looking examples too. it'll come back to me i'm sure.  update2: found it: http://weblogs.sqlteam.com/jeffs/archive/2008/04/23/unpivot.aspx",sql-server sql-server-2005
208,"the problem with leading wildcards: they cannot be indexed, hence you're doing a full table scan.",sql-server full-text-search
209,"you could try signal averaging, i.e. for each point, average the value with the surrounding 3 or more points. if the noise blips are huge, then even this may not help.    i realise that this was language agnostic, but guessing that you are using labview, there are lots of pre-packaged signal processing vis that come with labview that you can use to do smoothing and noise reduction. the ni forums are a great place to get more specialised help on this sort of thing.",algorithm language-agnostic
210," choose project -> properties select configuration -> general in the box for how you should link mfc, choose to statically link it. choose linker -> input.  under additional dependencies, add any libraries you need your app to statically link in.   for more info, see this article: http://www.geekadmin.com/ p=34 ",c++ visual-studio
211,"i use it, especially since the hosted version of fugbugz is free for up to 2 people. i found it a lot nicer than paper as i'm working on multiple projects, and my paper tends to get rather messy once you start making annotations or if you want to re-organize and shuffle tasks around, mark them as complete only to see that they are not complete after all...  plus, the visual studio integration is really neat, something paper just cannot compete with. also, if you lay the project to rest for 6 months and come back, all your tasks and notes are still there, whereas with paper you may need to search all the old documents and notes again, if you did not discard it.  but that is just the point of view from someone who is not really good at staying organized :-) if you are a really tidy and organized person, paper may work better for you than it does for me.  bonus suggestion: run fogbugz on a second pc (or a small laptop like the eeepc) so that you always have it at your fingertips. the main problem with task tracking programs - be it fogbugz, outlook, excel or just notepad - is that they take up screen space, and my two monitors are usually full with visual studio, e-mail, web browsers, some notepads etc. ",fogbugz
212,"the framework included with vs9 is .net, but you can write tests in c++/cli, so as long as you're comfortable learning some .net isms, you should be able to test most any c++ code.  boost.test  and googletest look to be fairly similar, but adapted for slightly different uses. both of these have a binary component, so you'll need an extra project in your solution to compile and run the tests.  the framework we use is cxxtest, which is much lighter; it's headers only, and uses a perl ( ) script to scrape test suite information from your headers (suites inherit from cxxtest::base, all your test methods' names start with ""test""). obviously, this requires that you get perl from one source or another, which adds overhead to your build environment setup. ",c++ visual-studio unit-testing testing
213,"i know this is a little late in the game, but the url mentioned for the javascript is mentioned in a list of sites known to have been part of the asprox bot resurgence that started up in june (at least that's when we were getting flagged with it). some details about it are mentioned below:    http://www.bloombit.com/articles/2008/05/ascii-encoded-binary-string-automated-sql-injection.aspx     the nasty thing about this is that effectively every varchar type field in the database is ""infected"" to spit out a reference to this url, in which the the browser gets an tiny iframe that turns it into a bot. a basic sql fix for this can be found here:    http://aspadvice.com/blogs/programming_shorts/archive/2008/06/27/asprox-recovery.aspx    the scary thing though is that the virus looks to the system tables for values to infect and a lot of shared hosting plans also share the database space for their clients. so most likely it wasn't even your dad's site that was infected, but somebody else's site within his hosting cluster that wrote some poor code and opened the door to sql injection attack.    if he hasn't done so yet, i'd send an urgent e-mail to their host and give them a link to that sql code to fix the entire system. you can fix your own affected database tables, but most likely the bots that are doing the infection are going to pass right through that hole again and infect the whole lot.    hopefully this gives you some more info to work with.    edit: one more quick thought, if he's using one of the hosts online design tools for building his website, all of that content is probably sitting in a column and was infected that way.",security cracking hijacked
214,"    and if it matters at all, apache@localhost is not the name of the user account that i use to get into the database. i don't have any user accounts with the name apache in them at all for that matter.      if it is saying 'apache@localhost' the username is not getting passed correctly to the mysql connection. 'apache' is normally the user that runs the httpd process (at least on redhat-based systems) and if no username is passed during the connection mysql uses whomever is calling for the connection.    if you do the connection right in your script, not in a called file, do you get the same error ",php mysql
215,"you can use the row_number() function. its used as follows:  select row_number() over(order by username) as rowid, userfirstname, userlastname from users   from which it will yield a result set with a rowid field which you can use to page between.  select *  from      ( select row_number() over(order by username) as rowid, userfirstname, userlastname       from users      ) as rowresults where rowid between 5 and 10   etc ",sql sql-server-2005 paging
216,"it's called alpha blending.    in psuedocode, assuming the background color (blend) always has 255 alpha. also assumes alpha is 0-255.    alpha=argb.alpha()r = (alpha/255)*argb.r() + (1 - alpha/255)*blend.r()g = (alpha/255)*argb.g() + (1 - alpha/255)*blend.g()b = (alpha/255)*argb.b() + (1 - alpha/255)*blend.b()    note: you probably need to be a bit (more) careful about floating-point/int math and rounding issues, depending on language. cast intermediates accordingly    edited to add:    if you don't have a background color with an alpha of 255, the algebra gets alot more complicated. i've done it before and it's a fun exercise left to the reader (if you really need to know, ask another question :).     in other words, what color c blends into some background the same as blending a, then blending b. this is sort of like calculating a+b (which isn't the same as b+a).",c# colors
217,you need to handle the system.windows.forms.application.threadexception event for windows forms. this article really helped me: http://bytes.com/forum/thread236199.html. ,winforms error-handling user-controls
218,"well, one of the reaons why classes usually contain private members with public getters/setters is exactly because they can verify data.    if you have a number than can be between 1 and 100, i would definitely put something in the setter that validates that and then maybe throw an exception that is being caught by the code. the reason is simple: if you don't do it in the setter, you have to remember that 1 to 100 limitation every time you set it, which leads to duplicated code or when you forget it, it leads to an invalid state.    as for performance, i'm with knuth here:        ""we should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil.""  ",optimization setter getter verification
219,"there's an out-twitter script i use for posting to twitter. it's nice, as it means you can send something to twitter without the risk of being distracted by a browser.    i added an alias for it, ""twit"".    so now you can type, for example:    ps c:\&gt;""trying out stack overflow"" | twit    and if successfully lodged, it will return an integer that identifies your post.",powershell cmdlets
220,"flash is certainly the most ubiquitous and portable solution.  98% of browsers have flash installed.  other alternatives are quicktime, windows media player, or even silverlight (microsoft's flash competitor, which can be used to embed several video formats).    i would recommend using flash (and it's flv video file format) for embedding your video unless you have very specific requirements as far as video quality or drm.",asp.net flash video embed
221,the glib library used on the gnome project may also be some use. moreover it is pretty well tested.  ibm developer works has a good tutorial on its use: manage c data using the glib collections ,c architecture data-structures
222,"firefox 30 ignores autocomplete=""off"" for passwords, opting to prompt the user instead whether the password should be stored on the client. note the following commentary from may 5, 2014:        the password manager always prompts if it wants to save a password. passwords are not saved without permission from the user.   we are the third browser to implement this change, after ie and chrome.      according to mozilla developer documentation the form element attribute autocomplete prevents form data from being cached in older browsers.  &lt;input type=""text"" name=""foo"" autocomplete=""off"" /&gt;  ",html autocomplete
223,"something like this   dim cars(2),x cars(0)=""volvo"" cars(1)=""saab"" cars(2)=""bmw""  for each x in cars   response.write(x &amp; ""&lt;br /&gt;"") next   see www.w3schools.com.  if you want to associate keys and values use a dictionary object instead:  dim objdictionary set objdictionary = createobject(""scripting.dictionary"") objdictionary.add ""name"", ""scott"" objdictionary.add ""age"", ""20"" if objdictionary.exists(""name"") then     ' do something else     ' do something else  end if  ",asp-classic vbscript
224,"it really depends on the sort of data you're bringing back.  since a dataset is (in effect) just a collection of datatable objects, you can return multiple distinct sets of data into a single, and therefore more manageable, object.    performance-wise, you're more likely to get inefficiency from unoptimized queries than from the ""wrong"" choice of .net construct.  at least, that's been my experience. ",c# dataset datatable
225,"i have found the solution else where:    select substring(master.dbo.fn_varbintohexstr(hashbytes('md5', 'helloworld')), 3, 32) ",sql sql-server
226,"i have only really used pl/pgsql, but that was because i needed a few stored procedures relatively fast and didn't want to add extra modules to the server.  longer term, i would probably use pl/perl or pl/python, as i use perl for quick scripting and have been looking at python for a while now.  one thing i have found is that there is a lack of good documentation for that on the postgresql site. the manuals were thorough as a reference, but did not work well as a tutorial to help show people how it should be done.  that, combined with a very good debugging environment, meant that my first experience of writing procedures involved looking at weird syntax errors for a long time.  if someone knows of a good site with tutorials etc for postgresql programming, i would love to get a link to it. ",sql database postgresql stored-procedures triggers
227,"this is more or less the perfect use case for svk.  svk is a command line front end for subversion that works with an entire local copy of the repository.  so your commits, updates, etc. work on the local repository and you can then sync with a master.  i would generally recommend svk over plain subversion anyway as it makes a lot of things nicer.  no .svn folders, better branching and merging, better conflict resolution. ",svn version-control
228,"there are so many it's crazy.  crazy good, i guess.   for the conservative types (me), nunit is still available and still more than capable. for the microsoft-types, mstest is adequate, but slow and clunky compared to nunit.  it also lacks code coverage without paying the big bucks for the pricey versions of visual studio. there's also mbunit.  it's like nunit, but has nifty features like rowtest (run the same test with different parameters) and rollback (put the database back like you found it after a test)  and finally, xunit.net is the trendy option with some attitude. oh, and testdriven.net will give you ide integration for both nunit and mbunit.   i'm sure they're all just fine.  i'd steer away from mstest though, unless you just enjoy the convenience of having everything in one ide out of the box.  scott hanselman has a podcast on this very topic. ",c# .net unit-testing testing
229,"you can not do this programatically (in a safe way that is) without creating a new table.   what enterprise manager does when you commit a reordering is to create a new table, move the data and then delete the old table and rename the new table to the existing name.   if you want your columns in a particular order/grouping without altering their physical order, you can create a view which can be whatever you desire.",sql-server
230,"it is possible to skip the step of creating the empty database. you can create the new database as part of the restore process.    this is actually the easiest and best way i know of to clone a database. you can eliminate errors by scripting the backup and restore process rather than running it through the sql server management studio    there are two other options you could explore:      detach the database, copy the .mdf file and re-attach.  use sql server integration services (ssis) to copy all the objects over      i suggest sticking with backup and restore and automating if necessary.",sql-server sql-server-2008
231,"not sure how credible this source is, but:     the windows server 2008 core edition can:         run the file server role.   run the hyper-v virtualization server role.   run the directory services role.   run the dhcp server role.   run the iis web server role.   run the dns server role.   run active directory lightweight directory services.   run the print server role.         the windows server 2008 core edition cannot:         run a sql server.   run an exchange server.   run internet explorer.   run windows explorer.   host a remote desktop session.   run mmc snap-in consoles locally.     ",sql-server windows-server-2008
232,"if you are quite happy with msbuild, then i would stick with msbuild.  this may be one of those cases where the tool you learn first is the one you will prefer.  i started with nant and can't quite get used to msbuild.  i'm sure they will both be around for quite some time.    there are some fundamental differences between the two, probably best highlighted by this conversation between some nant fans and a microsoftie.    interestingly, jeremy miller asked the exact opposite question on his blog last year.  ",.net .net-3.5 msbuild nant
233,"the way we do this is to have a 'catch all' for our domain name registered in dns so that anything.ourdomain.com will point to our server.  with apache you can set up a similar catch-all for your vhosts.  the servername must be a single static name but the serveralias directive can contain a pattern.  servername www.ourdomain.com serveralias *.ourdomain.com   now all of the domains will trigger the vhost for our project.  the final part is to decode the domain name actually used so that you can work out the username in your code, something like (php):  list( $username ) = explode( ""."", $_server[ ""http_host"" ] );   or a rewriterule as already suggested that silently maps user.ourdomain.com/foo/bar to www.ourdomain.com/foo/bar user=user or whatever you prefer. ",web-applications dns subdomain account
234,"you have basically two options. either you write a simple script that will start and parse output from  svn.exe info --revision head  to obtain revision number (then generating assemblyinfo.cs is pretty much straight forward) or just use plugin for ccnet. here it is:     svn revision labeller is a plugin for   cruisecontrol.net that allows you to   generate cruisecontrol labels for your   builds, based upon the revision number   of your subversion working copy. this   can be customised with a prefix and/or   major/minor version numbers.      http://code.google.com/p/svnrevisionlabeller/   i prefer the first option because it's only roughly 20 lines of code:  using system; using system.diagnostics;  namespace svnrevisionnumberparsersample {     class program     {         static void main()         {             process p = process.start(new processstartinfo()                 {                     filename = @""c:\program files\sliksvn\bin\svn.exe"", // path to your svn.exe                     useshellexecute = false,                     redirectstandardoutput = true,                     arguments = ""info --revision head"",                     workingdirectory = @""c:\myproject"" // path to your svn working copy                 });              // command ""svn.exe info --revision head"" will produce a few lines of output             p.waitforexit();              // our line starts with ""revision: ""             while ( p.standardoutput.endofstream)             {                 string line = p.standardoutput.readline();                 if (line.startswith(""revision: ""))                 {                     string revision = line.substring(""revision: "".length);                     console.writeline(revision); // show revision number on screen                                            break;                 }             }              console.read();         }     } }  ",svn msbuild cruisecontrol.net
235,"the exact definitions of ""vsize,"" ""rss,"" ""rprvt,"" ""rshrd,"" and other obscure-looking abbreviations vary from os to os.  the manual pages for the ""top"" and ""ps"" commands will have some sort of description, but all such descriptions are simplified greatly (or are based on long-extinct kernel implementations).  ""process size"" as a concept is fiendishly difficult to pin down in the general case.  answers in specific instances depend heavily on the actual memory management implementation in the os, and are rarely as satisfying as the tidy ""process size"" concept that exists in the minds of most users (and most developers).    for example, none of those numbers (nor, likely, any combination of them) can be used to tell you exactly how many such processes can run at once in a given amount of free memory.  but really, your best bet is to come at it from that end: why do you want this number, and what will you use it for   given that information, i think you'll get more useful answers.",unix size
236,"killing other processes is not a healthy thing to do. if your scenario involves something like uninstallation, you could use the movefileex api function to mark the file for deletion upon next reboot.  if it appears that you really need to delete a file in use by another process, i'd recommend re-considering the actual problem before considering any solutions. ",c# file-locking
237,"rex, i suspect a good place to start looking is solutions that compress the viewstate -- they're grabbing viewstate on the server before it's sent down to the client and gzipping it. that's exactly where you want to be.      scott hanselman on viewstate compression (2005)  viewstate compression with system.io.compression (2007)  ",c# asp.net
238,"@goyuix -- that's excellent for something written from memory. tested it here -- found the connection wasn't opened. otherwise very nice.  using system.data.oledb; ...  using (oledbconnection conn = new oledbconnection()) {     conn.connectionstring = ""provider=sqloledb;data source=yourservername\\yourinstance;initial catalog=databasename;integrated security=sspi;"";      using (oledbcommand cmd = new oledbcommand())     {         conn.open();         cmd.connection = conn;         cmd.commandtext = ""select * from yourtable"";          using (oledbdatareader dr = cmd.executereader())         {             while (dr.read())             {                 console.writeline(dr[""columnname""]);             }         }     } }  ",c# database loops connection
239,"i'm guessing that because you want to set it for all users, that you're on some kind of shared computer, which is probably running under a domain     here be dragons    let's say joe and jane regularly log onto the computer, then they will each have 'registries'.    you'll then install your app, and the installer will employ giant hacks and disgusting things to set items under hkcu for them.    then, bob will come along and log on (he, and 500 other people have accounts in the domain and so can do this). he's never used this computer before, so he has no registry. the first time he logs in, windows creates him one, but he won't have your setting.     your app then falls over or behaves incorrectly, and bob complains loudly about those crappy products from raynixon incorporated.    the correct answer is to just have some default settings in your app, which can write them to the registry if it doesn't find them. it's general good practice that your app should never depend on the registry, and should create things as needed, for any registry entry, not just hkcu, anyway",windows visual-studio registry installation
240,"with the ""built in"" stuff, you can't, as using 1.0.* or 1.0.0.* will replace the revision and build numbers with a coded date/timestamp, which is usually also a good way.   for more info, see the assembly linker documentation in the /v tag.  as for automatically incrementing numbers, use the assemblyinfo task:  assemblyinfo task  this can be configured to automatically increment the build number.  there are 2 gotchas:   each of the 4 numbers in the version string is limited to 65535. this is a windows limitation and unlikely to get fixed.  why are build numbers limited to 65535   using with with subversion requires a small change:  using msbuild to generate assembly version info at build time (including subversion fix)    retrieving the version number is then quite easy:  version v = assembly.getexecutingassembly().getname().version; string about = string.format(cultureinfo.invariantculture, @""yourapp version {0}.{1}.{2} (r{3})"", v.major, v.minor, v.build, v.revision);     and, to clarify: in .net or at least in c#, the build is actually the third number, not the fourth one as some people (for example delphi developers who are used to major.minor.release.build) might expect.  in .net, it's major.minor.build.revision. ",c# visual-studio versioning
241,"i've taken to hand-coding all of my ddl (creates/alter/delete) statements, adding them to my .sln as text files, and using normal versioning (using subversion, but any revision control should work). this way, i not only get the benefit of versioning, but updating live from dev/stage is the same process for code and database - tags, branches and so on work all the same.    otherwise, i agree redgate is expensive if you don't have a company buying it for you. if you can get a company to buy it for you though, it really is worth it ",sql-server sql-server-2005 deployment release-management
242,"no, what you're doing is fine. don't let those people confuse you.    if you've written the web services with .net then the reference proxies generated by .net are going to be quite suitable. the situation you describe (where you are both producer and consumer) is the ideal situation.    if you need to connect to a web services that is unknown at compile time, then you would want a more dynamic approach, where you deduce the 'shape' of the web service.     but start by using the auto generated proxy class, and don't worry about it until you hit a limitation. and when you do -- come back to stack overflow ;-)",.net web-services
243,"i would be a bit reluctant to use nested classes here.  what if you created an abstract base class for a ""multimedia driver"" to handle the back-end stuff (workhorse), and a separate class for the front-end work   the front-end class could take a pointer/reference to an implemented driver class (for the appropriate media type and situation) and perform the abstract operations on the workhorse structure.  my philosophy would be to go ahead and make both structures accessible to the client in a polished way, just under the assumption they would be used in tandem.  i would reference something like a qtextdocument in qt.  you provide a direct interface to the bare metal data handling, but pass the authority along to an object like a qtextedit to do the manipulation. ",c++ oop class nested-class
244,"oleg shilo's c# script solution (at the code project) really is a great introduction to providing script abilities in your application.  a different approach would be to consider a language that is specifically built for scripting, such as ironruby, ironpython, or lua.  ironpython and ironruby are both available today.  for a guide to embedding ironpython read how to embed ironpython script support in your existing app in 10 easy steps.  lua is a scripting language commonly used in games. there is a lua compiler for .net, available from codeplex -- http://www.codeplex.com/nua  that codebase is a great read if you want to learn about building a compiler in .net.  a different angle altogether is to try powershell. there are numerous examples of embedding powershell into an application -- here's a thorough project on the topic:  powershell tunnel ",c# .net scripting compiler-construction
245,"my first thought on this is ""how generate n vectors in a space that maximize distance from each other."" you can see that the rgb (or any other scale you use that forms a basis in color space) are just vectors. take a look at random point picking. hope this is a good start for you  once you have a set of vectors that are maximized a part, you can save them in a hash table or something for later, and just perform random rotations on them to get all the colors you desire that are maximally apart from each other   edit: thinking about this problem more, it would be better to map the colors in a linear manor, possibly (0,0,0) --> (255,255,255) lexicographically, and then distribute them evenly. i really don't know how well this will work, but it should since, lets say:  n = 10 we know we have 16777216 colors (256^3). we can use buckles algorithm 515 to find the lexicographically indexed color.. you'll probably have to edit the algorithm to avoid overflow and probably add some minor speed improvements. ",algorithm language-agnostic colors color-space
246,"version control with subversion    a very good resource for source control in general. not really tortoisesvn specific, though.",svn tortoisesvn branch branching-and-merging
247,"i wound up using this. it is a kind of a hack, but it actually works pretty well. the only thing is you have to be very careful with your semicolons. : d  var strsql:string = stream.readutfbytes(stream.bytesavailable);       var i:number = 0; var strsqlsplit:array = strsql.split("";""); for (i = 0; i &lt; strsqlsplit.length; i++){     nonquery(strsqlsplit[i].tostring()); }  ",flex actionscript-3 air
